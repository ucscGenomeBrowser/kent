#!/bin/csh -f # set emacs mode
exit; # don't actually run this like a script :)

# Drosophila Melanogaster -- 
# 
# Berkeley Drosophila Genome Project (fruitfly.org) release 3.1 (Jan. 2003)
# http://www.fruitfly.org/annot/release3.html
#
# FlyBase (http://flybase.bio.indiana.edu/) last updated 20 Jan 2003
#

# DOWNLOAD SEQUENCE (DONE 10/2/03 angie)
    ssh kksilo
    mkdir /cluster/store6/dm1
    cd /cluster/data
    ln -s /cluster/store6/dm1 dm1
    cd /cluster/data/dm1
    wget ftp://ftp.fruitfly.org/pub/download/dmel_RELEASE3-1/FASTA/whole_genome_genomic_dmel_RELEASE3-1.FASTA
    faSplit byname whole_genome_genomic_dmel_RELEASE3-1.FASTA dummyArg

    # Follow FlyBase's lead on the chromosome names, but still use our 
    # "chr" prefix:
    foreach c (2L 2R 2h 3L 3R 3h 4 X Xh Yh U)
      mkdir $c
      sed -e 's/^>/>chr/' $c.fa > $c/chr$c.fa
      echo chr$c.fa size:
      faSize $c.fa
      echo $c/chr$c.fa size:
      faSize $c/chr$c.fa
      echo comparison:
      faCmp $c.fa $c/chr$c.fa
      echo ""
    end
    # Carefully review output of those commands, then:
    rm 2L.fa 2R.fa 2h.fa 3L.fa 3R.fa 3h.fa 4.fa X.fa Xh.fa Yh.fa U.fa

    # put away the big download file
    mkdir -p downloads/fruitfly
    mv whole_genome_genomic_dmel_RELEASE3-1.FASTA downloads/fruitfly/


# SPLIT CHROM FA INTO SMALLER CHUNKS BY GAPS (DONE 10/3/03 angie)
    ssh kksilo
    cd /cluster/data/dm1
    foreach c (?{,?})
      faSplit -minGapSize=100 -lift=$c/chr$c.lft \
        gap $c/chr$c.fa 2000000 $c/chr${c}_
    end
    foreach ctgFa (?{,?}/chr*_*.fa)
      set ctg = $ctgFa:r
      mkdir $ctg
      mv $ctgFa $ctg
    end


# CREATING DATABASE (DONE 10/3/03 angie)
    # Create the database.
    ssh hgwdev
    echo 'create database dm1' | hgsql ''
    # Make a semi-permanent read-only alias:
    alias dm1 "mysql -u hguser -phguserstuff -A dm1"
    # Make sure there is at least 5 gig free for the database
    df -h /var/lib/mysql


# EXTRACT GAP INFORMATION FROM FASTA, LOAD GAP TRACK (DONE 10/3/03 angie)
    ssh kksilo
    cd /cluster/data/dm1
    # size up the gap situation -- can we use gaps to extract agp info?
    faGapSizes downloads/fruitfly/whole_genome_genomic_dmel_RELEASE3-1.FASTA 
    # yup.  Jim's verdict:
    # I think that we can probably just show all gaps as bridged
    # in the non-h chromosomes, and as unbridged in the h chromosomes
    # and leave it at that.

    # Extract gaps using scaffoldFaToAgp.  It's really meant for a different 
    # purpose, so clean up its output: remove the .lft and .agp, and remove 
    # the last line of .gap (extra gap added at end).  Also substitute in 
    # the correct chrom name in .gap.  
    foreach c (?{,?})
      set chr = chr$c
      pushd $c
      scaffoldFaToAgp -minGapSize=100 $chr.fa
      rm $chr.{lft,agp}
      set chrSize = `faSize $chr.fa | awk '{print $1;}'`
      set origLines = `cat $chr.gap | wc -l`
      awk '($2 != '$chrSize'+1) {print;}' $chr.gap \
      | sed -e "s/chrUn/$chr/" > $chr.gap2
      set newLines = `cat $chr.gap2 | wc -l`
      if ($newLines == ($origLines - 1)) then
        mv $chr.gap2 $chr.gap
      else
        echo "Error: $chr/$chr.gap2 has wrong number of lines."
      endif
      popd
    end
    # Call the gaps unbridged in chrU and chr*h:
    foreach c (U ?h)
      set chr = chr$c
      sed -e 's/yes/no/' $c/$chr.gap > $c/$chr.gap2
      mv $c/$chr.gap2 $c/$chr.gap
    end
    ssh hgwdev
    hgLoadGap dm1 /cluster/data/dm1


# MAKE DESCRIPTION/SAMPLE POSITION HTML PAGE (DONE 10/13/03 angie/donnak)
    ssh hgwdev
    # Write ~/kent/src/hg/makeDb/trackDb/drosophila/dm1/description.html 
    # with a description of the assembly and some sample position queries.  
    chmod a+r ~/kent/src/hg/makeDb/trackDb/drosophila/dm1/description.html
    # Check it in and copy (perhaps via a make in trackDb??) to 
    # /cluster/data/dm1/html.  
    mkdir -p /gbdb/dm1/html
    ln -s /cluster/data/dm1/html/description.html /gbdb/dm1/html/


# RUN REPEAT MASKER (DONE 10/4/03 angie)
    # Note: drosophila library ("drosophila.lib") is dated May 27 '03.
    # Contigs (*/chr*_*/chr*_*.fa) are split into 500kb chunks to make 
    # RepeatMasker runs manageable on the cluster ==> results need lifting.

    # Split contigs into 500kb chunks:
    ssh kksilo
    cd /cluster/data/dm1
    foreach d ( */chr*_?{,?} )
      cd $d
      set contig = $d:t
      faSplit -minGapSize=100 -lift=$contig.lft -maxN=500000 \
        gap $contig.fa 500000 ${contig}_
      cd ../..
    end

    # make the run directory, output directory, and job list
    mkdir RMRun
    cp /dev/null RMRun/RMJobs
    foreach d ( ?{,?}/chr*_?{,?} )
      set ctg = $d:t
      foreach f ( $d/${ctg}_?{,?}.fa )
        set f = $f:t
        echo /cluster/bin/scripts/RMDrosophila \
             /cluster/data/dm1/$d $f /cluster/data/dm1/$d \
           '{'check out line+ /cluster/data/dm1/$d/$f.out'}' \
        >> RMRun/RMJobs
      end
    end

    # do the run
    ssh kk
    cd /cluster/data/dm1/RMRun
    para create RMJobs
    para try
    para check
    para push
    para check,...
#Completed: 288 of 288 jobs
#CPU time in finished jobs:    1764726s   29412.10m   490.20h   20.43d  0.056 y
#IO & Wait Time:                  3392s      56.53m     0.94h    0.04d  0.000 y
#Average job time:                6139s     102.32m     1.71h    0.07d
#Longest job:                     7256s     120.93m     2.02h    0.08d
#Submission to last job:          7257s     120.95m     2.02h    0.08d

    # Lift up the split-contig .out's to contig-level .out's
    ssh kksilo
    cd /cluster/data/dm1
    foreach d ( ?{,?}/chr*_?{,?} )
      cd $d
      set contig = $d:t
      liftUp $contig.fa.out $contig.lft warn ${contig}_*.fa.out > /dev/null
      cd ../..
    end

    # Lift up the contig-level .out's to chr-level
    foreach c (?{,?})
      cd $c
      if (-e chr$c.lft && ! -z chr$c.lft) then
        echo lifting $c
        /cluster/bin/i386/liftUp chr$c.fa.out chr$c.lft warn \
          `awk '{print $2"/"$2".fa.out";}' chr$c.lft` > /dev/null
      else
        echo Can\'t find $c/chr$c.lft \!
      endif
      cd ..
    end

    # soft-mask contig .fa's with .out's
    foreach c (?{,?})
      foreach j ($c/chr${c}_?{,?}/chr${c}_?{,?}.fa)
        maskOutFa $j $j.out $j -soft
      end
      echo done $c
    end

    # Load the .out files into the database with:
    ssh hgwdev
    hgLoadOut dm1 /cluster/data/dm1/?{,?}/*.fa.out


# SIMPLE REPEATS (TRF) (DONE 10/5/03 angie)
    # TRF runs pretty quickly now... it takes a few hours total runtime, 
    # so instead of binrsyncing and para-running, just do this on the
    # local fileserver
    ssh kksilo
    mkdir /cluster/data/dm1/bed/simpleRepeat
    cd /cluster/data/dm1/bed/simpleRepeat
    mkdir trf
    cp /dev/null jobs.csh
    foreach f (/cluster/data/dm1/?{,?}/chr*_*/chr?{,?}_?{,?}.fa)
        set fout = $f:t:r.bed
        echo $fout
        echo "/cluster/bin/i386/trfBig -trf=/cluster/bin/i386/trf $f /dev/null -bedAt=trf/$fout -tempDir=/tmp" \
        >> jobs.csh
    end
    tcsh jobs.csh >&! jobs.log &
    # check on this with
    tail -f jobs.log
    wc -l jobs.csh
    ls -1 trf | wc -l
    # When job is done do:
    mkdir /cluster/data/dm1/jkStuff
    liftUp simpleRepeat.bed /cluster/data/dm1/jkStuff/liftAll.lft warn \
      trf/*.bed

    # Load this into the database as so
    ssh hgwdev
    hgLoadBed dm1 simpleRepeat \
      /cluster/data/dm1/bed/simpleRepeat/simpleRepeat.bed \
      -sqlTable=$HOME/src/hg/lib/simpleRepeat.sql


# FILTER SIMPLE REPEATS (TRF) INTO MASK (DONE 10/5/03 angie)
    # make a filtered version # of the trf output: 
    # keep trf's with period <= 12:
    ssh kksilo
    cd /cluster/data/dm1/bed/simpleRepeat
    mkdir -p trfMask
    foreach f (trf/*.bed)
        echo "filtering $f"
        awk '{if ($5 <= 12) print;}' $f > trfMask/$f:t
    end
    # Lift up filtered trf output to chrom coords as well:
    cd /cluster/data/dm1
    mkdir bed/simpleRepeat/trfMaskChrom
    foreach c (?{,?})
      liftUp bed/simpleRepeat/trfMaskChrom/chr$c.bed $c/chr$c.lft warn \
        `awk '{print "bed/simpleRepeat/trfMask/"$2".bed";}' $c/chr$c.lft`
    end


# MASK FA USING REPEATMASKER AND FILTERED TRF FILES (DONE 10/5/03 angie)
    ssh kksilo
    cd /cluster/data/dm1
    foreach c (?{,?})
      echo repeat- and trf-masking chr$c.fa
      /cluster/home/kent/bin/i386/maskOutFa -soft \
        $c/chr$c.fa $c/chr$c.fa.out $c/chr$c.fa
      /cluster/home/kent/bin/i386/maskOutFa -softAdd \
        $c/chr$c.fa bed/simpleRepeat/trfMaskChrom/chr$c.bed $c/chr$c.fa
    end
    foreach c (?{,?})
      echo repeat- and trf-masking contigs of chr$c
      foreach ctgFa ($c/chr*/chr${c}_?{,?}.fa)
        set trfMask=bed/simpleRepeat/trfMask/$ctgFa:t:r.bed
        /cluster/home/kent/bin/i386/maskOutFa -soft $ctgFa $ctgFa.out $ctgFa
        /cluster/home/kent/bin/i386/maskOutFa -softAdd $ctgFa $trfMask $ctgFa
      end
    end


# STORE SEQUENCE AND ASSEMBLY INFORMATION (DONE 10/5/03 angie)

    # Translate to nib
    ssh kksilo
    cd /cluster/data/dm1
    mkdir nib
    foreach c (?{,?})
      faToNib -softMask $c/chr$c.fa nib/chr$c.nib
    end

    # Make symbolic links from /gbdb/dm1/nib to the real nibs.
    ssh hgwdev
    mkdir -p /gbdb/dm1/nib
    foreach f (/cluster/data/dm1/nib/chr*.nib)
      ln -s $f /gbdb/dm1/nib
    end

    # Load /gbdb/dm1/nib paths into database and save size info.
    hgsql dm1  < ~/src/hg/lib/chromInfo.sql
    hgNibSeq -preMadeNib dm1 /gbdb/dm1/nib /cluster/data/dm1/?{,?}/chr?{,?}.fa
    echo "select chrom,size from chromInfo" | hgsql -N dm1 \
      > /cluster/data/dm1/chrom.sizes


# CREATING GRP TABLE FOR TRACK GROUPING (DONE 10/5/03 angie)
    # Copy all the data from the table "grp" 
    # in the existing database "rn1" to the new database
    ssh hgwdev
    echo "create table grp (PRIMARY KEY(NAME)) select * from rn1.grp" \
      | hgsql dm1


# MAKE GCPERCENT (DONE 10/5/03 angie)
     ssh hgwdev
     mkdir /cluster/data/dm1/bed/gcPercent
     cd /cluster/data/dm1/bed/gcPercent
     # create and load gcPercent table
     hgsql dm1  < ~/src/hg/lib/gcPercent.sql
     hgGcPercent dm1 ../../nib


# MAKE HGCENTRALTEST ENTRY AND TRACKDB TABLE FOR DROSOPHILA (DONE 10/8/03 angie)
    # Warning: must genome and organism fields must correspond
    # with defaultDb values
    echo 'INSERT INTO dbDb \
        (name, description, nibPath, organism, \
                defaultPos, active, orderKey, genome, scientificName, \
                htmlPath, hgNearOk) values \
        ("dm1", "Jan. 2003", "/gbdb/dm1/nib", "Fruitfly", \
               "chr2L:827700-845800", 1, 55, "Fruitfly", \
                "Drosophila melanogaster", "/gbdb/dm1/html/description.html", \
                0);' \
      | hgsql -h genome-testdb hgcentraltest
    echo 'INSERT INTO defaultDb (genome, name) values ("Fruitfly", "dm1");' \
      | hgsql -h genome-testdb hgcentraltest

    # Make trackDb table so browser knows what tracks to expect:
    ssh hgwdev
    cd ~/src/hg/makeDb/trackDb
    cvs up -d -P

    # Edit that makefile to add dm1 in all the right places and do
    make update

    # go public on genome-test
    #make alpha
    cvs commit makefile

    # Add trackDb directories
    mkdir drosophila
    mkdir drosophila/dm1
    cvs add drosophila
    cvs add drosophila/dm1
    cvs commit drosophila


# MAKE HGCENTRALTEST BLATSERVERS ENTRY FOR DROSOPHILA (DONE 10/17/03 angie)
    ssh hgwdev
    # Get appropriate hostname from cluster admins
    echo 'insert into blatServers values("dm1", "blat10", "17787", "1"); \
          insert into blatServers values("dm1", "blat10", "17786", "0");' \
      | hgsql -h genome-testdb hgcentraltest


# LOAD UP BDGP 3.1 ANNOTATIONS (DONE 10/8/03 angie)
    # fruitfly.org is the Berkeley Drosophila Genome Project.  
    # Their GFF annotations contain FlyBase IDs useful for cross-linking.
    ssh kksilo
    mkdir /cluster/data/dm1/bed/bdgpAnnotations
    cd /cluster/data/dm1/bed/bdgpAnnotations
    # Download all available annotations:
    wget ftp://ftp.fruitfly.org/pub/download/dmel_RELEASE3-1/GFF/whole_genome_3_UTR_dmel_RELEASE3-1.GFF.gz
    wget ftp://ftp.fruitfly.org/pub/download/dmel_RELEASE3-1/GFF/whole_genome_5_UTR_dmel_RELEASE3-1.GFF.gz
    wget ftp://ftp.fruitfly.org/pub/download/dmel_RELEASE3-1/GFF/whole_genome_CDS_dmel_RELEASE3-1.GFF.gz
    wget ftp://ftp.fruitfly.org/pub/download/dmel_RELEASE3-1/GFF/whole_genome_annotation_dmel_RELEASE3-1.GFF.gz
    wget ftp://ftp.fruitfly.org/pub/download/dmel_RELEASE3-1/GFF/whole_genome_exon_dmel_RELEASE3-1.GFF.gz
    wget ftp://ftp.fruitfly.org/pub/download/dmel_RELEASE3-1/GFF/whole_genome_intron_dmel_RELEASE3-1.GFF.gz
    wget ftp://ftp.fruitfly.org/pub/download/dmel_RELEASE3-1/GFF/whole_genome_noncoding-gene_dmel_RELEASE3-1.GFF.gz
    wget ftp://ftp.fruitfly.org/pub/download/dmel_RELEASE3-1/GFF/whole_genome_protein-coding-gene_dmel_RELEASE3-1.GFF.gz
    wget ftp://ftp.fruitfly.org/pub/download/dmel_RELEASE3-1/GFF/whole_genome_splice_site_dmel_RELEASE3-1.GFF.gz
    wget ftp://ftp.fruitfly.org/pub/download/dmel_RELEASE3-1/GFF/whole_genome_tRNA_dmel_RELEASE3-1.GFF.gz
    wget ftp://ftp.fruitfly.org/pub/download/dmel_RELEASE3-1/GFF/whole_genome_transcript_dmel_RELEASE3-1.GFF.gz
    wget ftp://ftp.fruitfly.org/pub/download/dmel_RELEASE3-1/GFF/whole_genome_translation_dmel_RELEASE3-1.GFF.gz
    wget ftp://ftp.fruitfly.org/pub/download/dmel_RELEASE3-1/GFF/whole_genome_transposable_element_dmel_RELEASE3-1.GFF.gz
    gunzip *.gz
    # Protein-coding genes...
    perl -wpe 's/^/chr/; s/translation/CDS/; \
               s/genegrp.*transgrp=(\S+);.*$/$1/; \
               s/genegrp.*transgrp=(\S+)$/$1/;' \
      whole_genome_protein-coding-gene_dmel_RELEASE3-1.GFF \
    > bdgpGene.gff
    # Loading a .tab file caused some lines with super-long go fields 
    # to be skipped.  Generating a .sql file with INSERTs works.
    perl -we 'while (<>) { \
                chop; @fields = split("\t"); \
                if ($fields[2] eq "gene") { \
                  @vars = split("; ", $fields[8]); \
                  $go = "";  $cdna_clone = ""; \
                  foreach $v (@vars) { \
                    @vv = split("=", $v); \
                    if ($vv[0] eq "name") { \
                      $bdgpName = $vv[1]; \
                    } elsif ($vv[0] eq "dbxref") { \
                      if ($vv[1] =~ /^GO:(\d+)/) { \
                        $go .= "$1,"; \
                      } elsif ($vv[1] =~ /FlyBase:(\w+)/) { \
                        $flybase = $1; \
                      } else { \
                        die "unrecognized dbxref $vv[1]"; \
                      } \
                    } elsif ($vv[0] eq "symbol") { \
                      $symbol = $vv[1]; \
                    } elsif ($vv[0] eq "cytorange") { \
                      $cytorange = $vv[1]; \
                    } elsif ($vv[0] eq "cdna_clone") { \
                      $cdna_clone .= "$vv[1],"; \
                    } elsif ($vv[0] eq "genegrp") { \
                    } else { \
                      die "unrecognized var $v" \
                    } \
                  } \
                  print "INSERT INTO bdgpGeneInfo VALUES ( \"$bdgpName\", \"$flybase\", \"$go\", \"$symbol\", \"$cytorange\", \"$cdna_clone\");\n"; \
                } \
              }' \
      whole_genome_protein-coding-gene_dmel_RELEASE3-1.GFF \
    > bdgpGeneInfo.sql
    # Proteins for coding genes:
    wget ftp://ftp.fruitfly.org/pub/download/dmel_RELEASE3-1/FASTA/whole_genome_translation_dmel_RELEASE3-1.FASTA.gz
    gunzip -c whole_genome_translation_dmel_RELEASE3-1.FASTA.gz \
    | perl -wpe 's/^>(pp-)*(\w+)-\w(\w).*/>$2-R$3/' \
    > bdgpGenePep.fa
    # load up coding genes, proteins and assoc. info:
    ssh hgwdev
    ldHgGene dm1 bdgpGene /cluster/data/dm1/bed/bdgpAnnotations/bdgpGene.gff
    hgPepPred dm1 generic bdgpGenePep \
      /cluster/data/dm1/bed/bdgpAnnotations/bdgpGenePep.fa
    hgsql dm1 < $HOME/src/hg/lib/bdgpGeneInfo.sql
    hgsql dm1 < /cluster/data/dm1/bed/bdgpAnnotations/bdgpGeneInfo.sql
    # Non-coding genes...
    perl -wpe 's/^/chr/; \
               s/genegrp.*transgrp=(\S+);.*$/$1/; \
               s/genegrp.*transgrp=(\S+)$/$1/;' \
      whole_genome_noncoding-gene_dmel_RELEASE3-1.GFF \
    > bdgpNonCoding.gff
    sed -e 's/bdgpGeneInfo/bdgpNonCodingInfo/' \
      ~/kent/src/hg/lib/bdgpGeneInfo.sql \
      > bdgpNonCodingInfo.sql
    perl -we 'while (<>) { \
                chop; @fields = split("\t"); \
                if (($fields[2] ne "exon") && \
                    ($fields[2] ne "transcript") && \
                    ($fields[2] ne "translation")) { \
                  @vars = split("; ", $fields[8]); \
                  $go = "";  $cdna_clone = ""; \
                  foreach $v (@vars) { \
                    @vv = split("=", $v); \
                    if ($vv[0] eq "name") { \
                      $bdgpName = $vv[1]; \
                    } elsif ($vv[0] eq "dbxref") { \
                      if ($vv[1] =~ /^GO:(\d+)/) { \
                        $go .= "$1,"; \
                      } elsif ($vv[1] =~ /FlyBase:(\w+)/) { \
                        $flybase = $1; \
                      } else { \
                        die "unrecognized dbxref $vv[1]"; \
                      } \
                    } elsif ($vv[0] eq "symbol") { \
                      $symbol = $vv[1]; \
                    } elsif ($vv[0] eq "cytorange") { \
                      $cytorange = $vv[1]; \
                    } elsif ($vv[0] eq "cdna_clone") { \
                      $cdna_clone .= "$vv[1],"; \
                    } elsif ($vv[0] eq "genegrp") { \
                    } else { \
                      die "unrecognized var $v" \
                    } \
                  } \
                  print "INSERT INTO bdgpNonCodingInfo VALUES ( \"$bdgpName\", \"$flybase\", \"$go\", \"$symbol\", \"$cytorange\", \"$cdna_clone\");\n"; \
                } \
              }' \
      whole_genome_noncoding-gene_dmel_RELEASE3-1.GFF \
    >> bdgpNonCodingInfo.sql
    ssh hgwdev
    ldHgGene dm1 bdgpNonCoding \
      /cluster/data/dm1/bed/bdgpAnnotations/bdgpNonCoding.gff
    hgsql dm1 < /cluster/data/dm1/bed/bdgpAnnotations/bdgpNonCodingInfo.sql


# FLYBASE GENES (REDONE 6/28/04 angie)
    ssh hgwdev
    mkdir /cluster/data/dm1/bed/flyBase
    cd /cluster/data/dm1/bed/flyBase
    wget -O genes.txt.040617 \
      ftp://flybase.bio.indiana.edu/flybase/genes/genes.txt
    # Had to edit genes.txt.040617 to get around these two lines that didn't 
    # start with acode symbols:
    #line 258894
#Allele class: hypom
    #line 459025 (459026 of original)
#uncertain *u FBan0017679; annotated data are available for this gene.
    #line 3128725 (3128726 of original), for Trn-SR / FBgn0031456:
#%f nu
    hgFlyBase dm1 genes.txt.040617


# BDGP GENE DISRUPTION PROJECT/PSCREEN (DONE 7/29/04 angie)
    ssh hgwdev
    mkdir /cluster/data/dm1/bed/pscreen
    cd /cluster/data/dm1/bed/pscreen
    # Robin Hiesinger emailed 2 table dumps that need to be joined on ID;
    # saved as atbloom.dump, genetag_gdp.dump.
    # Turns out that this file has a more complete mapping than atbloom --
    # use it to fill in stock numbers missing from atbloom:
    wget http://flybase.bio.indiana.edu/stocks/stock-centers/bloomington/lk/bloomington.csv
    dos2unix atbloom.dump
    # one-shot Perl script:
    chmod a+x mkPscreen.pl
    mkPscreen.pl genetag_gdp.dump atbloom.dump bloomington.csv > pscreen.bed
    hgLoadBed dm1 pscreen -sqlTable=$HOME/kent/src/hg/lib/pscreen.sql -tab \
      pscreen.bed


# FLYBASE IN SITU IMAGES / EXPRESSION (REDONE 6/17/04 angie)
    # FlyBase has downloadable in situ images for BACs:
    # ftp://flybase.net/flybase/images/bac_insitu_pic/*
    # and FBti's:
    # ftp://flybase.net/flybase/images/in-situ-images/insitus.zip
    # but what Jim is interested in is the insitus for expression data.
    # Don't see that in the ftp listing, but they do make it easy to link in.
    ssh hgwdev
    cd /cluster/data/dm1/bed/flyBase
    wget -O summary.txt.040617 \
    'http://www.fruitfly.org/cgi-bin/ex/bquery.pl?qpage=entry&qtype=summarytext'
    hgsql dm1 < ~/kent/src/hg/lib/bdgpExprLink.sql
    hgsql dm1 -e \
      'load data local infile "summary.txt.040617" into table bdgpExprLink'


# SWISSPROT-FLYBASE CROSS-REFERENCING  (REDONE 6/17/04 angie)
    ssh hgwdev
    mkdir /cluster/data/dm1/bed/flyBaseSwissProt
    cd /cluster/data/dm1/bed/flyBaseSwissProt
    echo "select extAcc1,acc from extDbRef,extDb where extDbRef.extDb = extDb.id and extDb.val = 'flybase'" \
    | hgsql -N sp040515 \
    > fbSpAcc.tab
    ssh kksilo
    cd /cluster/data/dm1/bed/flyBaseSwissProt
    wget ftp://ftp.ebi.ac.uk/pub/databases/SPproteomes/fasta_files/proteomes/7227.FASTAC
    # Some of those SwissProt "names" are > 255 chars!  trim the few long ones.
    perl -we 'open(F, "fbSpAcc.tab") || die; \
              %sp2fb = (); \
              while (<F>) { \
                chop;  @words = split("\t"); \
                $sp2fb{$words[1]} = $words[0]; \
              } \
              close(F); \
              while (<>) { \
                if (/^>(\w+)\s+\((\w+)\)\s+(.*)/) { \
                  $fbAcc = $sp2fb{$2}; \
                  $spAcc = $3; \
                  $spAcc = substr($3, 0, 250) . "..." if (length($3) > 255); \
                  print "$fbAcc\t$2\t$spAcc\t$1\n" if (defined $fbAcc); \
                } \
              }' \
      7227.FASTAC \
    > flyBaseSwissProt.tab
    rm 7227.FASTAC
    ssh hgwdev
    hgsql dm1 < $HOME/src/hg/lib/flyBaseSwissProt.sql
    echo 'load data local infile "/cluster/data/dm1/bed/flyBaseSwissProt/flyBaseSwissProt.tab" into table flyBaseSwissProt' \
    | hgsql dm1


# AUTO UPDATE GENBANK MRNA RUN  (DONE 10/9/03 angie)

    # Put the nib's on /cluster/bluearc:
    ssh kksilo
    mkdir /cluster/bluearc/drosophila
    mkdir /cluster/bluearc/drosophila/dm1
    cp -pR /cluster/data/dm1/nib /cluster/bluearc/drosophila/dm1

    # Instructions for setting up incremental genbank updates are here:
    # http://www.soe.ucsc.edu/~markd/genbank-update/doc/initial-load.html
    # This time around, Markd handled adding the new species to gbGenome.c 
    # because it's not yet in the kent tree.  

    # Edit /cluster/data/genbank/etc/genbank.conf and add:
# dm1
dm1.genome = /cluster/bluearc/drosophila/dm1/nib/chr*.nib
dm1.lift = /cluster/data/dm1/jkStuff/liftAll.lft
dm1.genbank.mrna.xeno.load = yes
dm1.genbank.est.xeno.load = no
dm1.downloadDir = dm1

    ssh eieio
    cd /cluster/data/genbank
    # This is an -initial run, mRNA only:
    nice bin/gbAlignStep -iserver=no -clusterRootDir=/cluster/bluearc/genbank \
      -srcDb=genbank -type=mrna -verbose=1 -initial dm1

    # Load the results from the above
    ssh hgwdev
    cd /cluster/data/genbank
    nice bin/gbDbLoadStep -verbose=1 -drop -initialLoad dm1

    ssh eieio
    # To get this next one started, the work directory of the initial run 
    # needs to be moved out of the way.  
    rm -r /cluster/bluearc/genbank/work/initial.dm1
    # Now align refseqs:
    cd /cluster/data/genbank
    nice bin/gbAlignStep -iserver=no -clusterRootDir=/cluster/bluearc/genbank \
      -srcDb=refseq -type=mrna -verbose=1 -initial dm1
    # Load results:
    ssh hgwdev
    cd /cluster/data/genbank
    nice bin/gbDbLoadStep -verbose=1 dm1

    ssh eieio
    # To get this next one started, the work directory of the initial run 
    # needs to be moved out of the way.  
    rm -r /cluster/bluearc/genbank/work/initial.dm1
    # Now align ESTs:
    nice bin/gbAlignStep -iserver=no -clusterRootDir=/cluster/bluearc/genbank \
      -srcDb=genbank -type=est -verbose=1 -initial dm1
    # Load results:
    ssh hgwdev
    cd /cluster/data/genbank
    nice bin/gbDbLoadStep -verbose=1 dm1
    # Clean up:
    rm -r /cluster/bluearc/genbank/work/initial.dm1


# PUT NIBS ON ISCRATCH (DONE 10/9/03 angie)
    ssh kkr1u00
    cd /iscratch/i/dm1
    cp -pR /cluster/data/dm1/nib .
    iSync


# PRODUCING FUGU FISH ALIGNMENTS  (DONE 10/9/03 angie)
    # Assumes masked NIBs have been prepared as above
    # and Fugu pieces are already on kluster /iscratch/i.
    # next machine
    ssh kk
    mkdir -p /cluster/data/dm1/bed/blatFugu
    cd /cluster/data/dm1/bed/blatFugu
    mkdir psl
    ls -1S /iscratch/i/fugu/*.fa > fugu.lst
    ls -1S /iscratch/i/dm1/nib/chr*.nib > fly.lst
    cat << '_EOF_' > gsub
#LOOP
/cluster/bin/i386/blat -q=dnax -t=dnax -mask=lower {check in exists+ $(path1)} {check in line+ $(path2)} {check out line+ psl/$(root1)_$(root2).psl}
#ENDLOOP
'_EOF_'
    # << this line keeps emacs coloring happy
    gensub2 fly.lst fugu.lst gsub spec
    para create spec
    para try
    para check
    para push
    para check
#Completed: 1408 of 1408 jobs
#Average job time:                 217s       3.61m     0.06h    0.00d
#Longest job:                     1431s      23.85m     0.40h    0.02d
#Submission to last job:          1433s      23.88m     0.40h    0.02d

    # When cluster run is done, sort alignments
    # append _blatFugu to chrom for .psl file names.
    ssh kksilo
    cd /cluster/data/dm1/bed/blatFugu
    foreach c (2L 2R 2h 3L 3R 3h 4 X Xh Yh U)
      echo -n "chr${c} "
      pslCat psl/chr${c}_*.psl \
      | pslSortAcc nohead chrom temp stdin
      rm -f chrom/chr${c}_blatFugu.psl
      mv chrom/chr${c}.psl chrom/chr${c}_blatFugu.psl
    end

    # Load alignments
    ssh hgwdev
    cd /cluster/data/dm1/bed/blatFugu/chrom
    hgLoadPsl -noTNameIx dm1 chr*_blatFugu.psl

    # Make fugu /gbdb/ symlink and load Fugu sequence data.
    mkdir /gbdb/dm1/fuguSeq
    ln -s /cluster/store3/fuguSeq/fugu_v3_mask.fasta /gbdb/dm1/fuguSeq
    # ! ! !  DO NOT RUN hgLoadSeq in /gbdb - it leaves .tab files
    cd /cluster/data/dm1/bed/blatFugu
    hgLoadSeq dm1 /gbdb/dm1/fuguSeq/fugu_v3_mask.fasta


# BLASTZ D.PSEUDOOBSCURA (DONE 11/14/03 angie)
    ssh kksilo
    mkdir /cluster/data/dm1/bed/blastz.dp1.2003-11-14
    cd /cluster/data/dm1/bed/blastz.dp1.2003-11-14
    cat << '_EOF_' > DEF
# D.melanogaster vs. D.pseudoobscura
export PATH=/usr/bin:/bin:/usr/local/bin:/cluster/home/angie/schwartzbin:/cluster/bin/i386

ALIGN=blastz-run
BLASTZ=blastz
BLASTZ_H=2000
#BLASTZ_ABRIDGE_REPEATS=1 if SMSK is specified
BLASTZ_ABRIDGE_REPEATS=0

# TARGET - D. melanogaster
SEQ1_DIR=/cluster/bluearc/drosophila/dm1/nib
# unused: SEQ1_RMSK=
SEQ1_SMSK=
SEQ1_FLAG=-drosophila
SEQ1_IN_CONTIGS=0
SEQ1_CHUNK=10000000
SEQ1_LAP=10000

# QUERY - D. pseudoobscura
SEQ2_DIR=/cluster/bluearc/drosophila/dp1/trfFa
# unused: SEQ2_RMSK=
SEQ2_SMSK=
SEQ2_FLAG=-drosophila
SEQ2_IN_CONTIGS=0
SEQ2_CHUNK=10000000
SEQ2_LAP=0

BASE=/cluster/data/dm1/bed/blastz.dp1.2003-11-14

DEF=$BASE/DEF
RAW=$BASE/raw
CDBDIR=$BASE
SEQ1_LEN=$BASE/S1.len
SEQ2_LEN=$BASE/S2.len

#DEBUG=1
'_EOF_'
    # << this line keeps emacs coloring happy
    # run bash shell if you don't already:
    bash
    source DEF
    mkdir run
    ~angie/hummus/make-joblist $DEF > $BASE/run/j
    sh ./xdir.sh
    cd run
    sed -e 's#^#/cluster/home/angie/schwartzbin/#' j > j2
    wc -l j*
    head j2
    mv j2 j
    # cluster run
    ssh kk
    cd /cluster/data/dm1/bed/blastz.dp1.2003-11-14/run
    para create j
    para try, check, push, check, ....
#Completed: 15939 of 15939 jobs
#Average job time:                  16s       0.27m     0.00h    0.00d
#Longest job:                      287s       4.78m     0.08h    0.00d
#Submission to last job:           829s      13.82m     0.23h    0.01d

    # back in the bash shell on kksilo...
    mkdir /cluster/data/dm1/bed/blastz.dp1.2003-11-14/run.1
    cd /cluster/data/dm1/bed/blastz.dp1.2003-11-14/run.1
    ~angie/hummus/do.out2lav $DEF > j
    # small cluster run
    ssh kkr1u00
    cd /cluster/data/dm1/bed/blastz.dp1.2003-11-14/run.1
    para create j
    para try, check, push, check, ....
#Completed: 21 of 21 jobs
#Average job time:                  44s       0.73m     0.01h    0.00d
#Longest job:                       71s       1.18m     0.02h    0.00d
#Submission to last job:           111s       1.85m     0.03h    0.00d
    cd ..
    rm -r raw

    # Translate .lav to axt, with dp1 in scaffold coords for collaborators:
    ssh kksilo
    # oops, lavToAxt relies on nib (not fa), so make dp1 scaffoldNib first:
    mkdir /cluster/data/dp1/scaffoldsNib
    foreach f (/cluster/data/dp1/scaffolds/*.fa)
      faToNib -softMask $f /cluster/data/dp1/scaffoldsNib/$f:t:r.nib
    end
    cd /cluster/data/dm1/bed/blastz.dp1.2003-11-14
    mkdir axtScaffold
    foreach c (lav/*)
      pushd $c
      set chr=$c:t
      set out=axtScaffold/$chr.axt
      echo "Translating $chr lav to $out"
      cat `ls -1 *.lav | sort -g` \
        | lavToAxt stdin /cluster/data/dm1/nib /cluster/data/dp1/scaffoldsNib \
            stdout \
        | axtSort stdin ../../$out
      popd
    end
    # Lift query coords to chrom-level for browser display
    mkdir axtChrom
    foreach f (axtScaffold/chr*.axt)
      liftUp -axtQ axtChrom/$f:t /cluster/data/dp1/jkStuff/liftAll.lft warn $f
    end


# CHAIN & NET DM1/DP1 BLASTZ ALIGNMENTS (DONE 11/24/03 angie)
    ssh kkr1u00
    cd /cluster/data/dm1/bed/blastz.dp1.2003-11-14
    mkdir -p axtChainScaffoldQ/run1
    cd axtChainScaffoldQ/run1
    ls -1S ../../axtScaffold/*.axt > input.lst
    echo '#LOOP' > gsub
    echo 'doChain {check in exists $(path1)} {check out line+ chain/$(root1).chain} {check out line+ out/$(root1).out}' >> gsub
    echo '#ENDLOOP' >> gsub
    gensub2 input.lst single gsub spec
    echo '#\!/bin/csh' > doChain
    echo 'axtChain $1 /cluster/data/dm1/nib /cluster/data/dp1/scaffoldsNib $2 > $3' \
    >> doChain
    chmod a+x doChain
    mkdir chain out
    para create spec
    para try, check, push, check, ...
#Completed: 11 of 11 jobs
#Average job time:                  27s       0.45m     0.01h    0.00d
#Longest job:                       49s       0.82m     0.01h    0.00d
#Submission to last job:            49s       0.82m     0.01h    0.00d

    ssh kksilo
    cd /cluster/data/dm1/bed/blastz.dp1.2003-11-14/axtChainScaffoldQ
    chainMergeSort run1/chain/*.chain > all.chain
    chainSplit chain all.chain
    rm run1/chain/*.chain

    mkdir -p ../axtChainChrom/chain
    foreach f (chain/chr*.chain)
      liftUp -chainQ ../axtChainChrom/chain/$f:t \
        /cluster/data/dp1/jkStuff/liftAll.lft warn $f
    end
    liftUp -chainQ ../axtChainChrom/all.chain \
      /cluster/data/dp1/jkStuff/liftAll.lft warn all.chain

    ssh hgwdev
    cd /cluster/data/dm1/bed/blastz.dp1.2003-11-14/axtChainChrom/chain
    foreach i (*.chain)
      set c = $i:r
      hgLoadChain dm1 ${c}_chainDp1 $i
      echo done $c
    end

    # Create the nets.  You can do this while the database is loading
    ssh kksilo
    cd /cluster/data/dm1/bed/blastz.dp1.2003-11-14/axtChainScaffoldQ
    # Oops, need a dp1/scaffold.sizes
    cp /dev/null /cluster/data/dp1/scaffold.sizes
    foreach f (/cluster/data/dp1/scaffolds/*.fa)
      set s = `faSize $f | awk '{print $1;}'`
      echo "$f:t:r\t$s" >> /cluster/data/dp1/scaffold.sizes
    end
    # First do a crude filter that eliminates many chains so the
    # real chainer has less work to do.
    mkdir preNet
    cd chain
    foreach i (*.chain)
      echo preNetting $i
      chainPreNet $i /cluster/data/dm1/chrom.sizes \
        /cluster/data/dp1/scaffold.sizes ../preNet/$i
    end
    cd ..
    # Run the main netter, putting the results in n1.
    mkdir n1 
    cd preNet
    foreach i (*.chain)
      set n = $i:r.net
      echo primary netting $i
      chainNet $i -minSpace=1 /cluster/data/dm1/chrom.sizes \
        /cluster/data/dp1/scaffold.sizes ../n1/$n /dev/null
    end
    cd ..
    # Classify parts of net as syntenic, nonsyntenic etc.
    cat n1/*.net | netSyntenic stdin noClass.net

    # The final step of net creation needs the database.
    # Best to wait for the database load to finish if it
    # hasn't already.
    ssh hgwdev
    cd /cluster/data/dm1/bed/blastz.dp1.2003-11-14/axtChainScaffoldQ
    netClass -liftQ=/cluster/data/dp1/jkStuff/liftAll.lft -noAr \
      noClass.net dm1 dp1 fruitfly.net
    rm -r n1 noClass.net
    netFilter -minGap=10 fruitfly.net > fruitflyFilt.net
    liftUp -netQ ../axtChainChrom/fruitflyFiltChrom.net \
      /cluster/data/dp1/jkStuff/liftAll.lft warn fruitflyFilt.net
    hgLoadNet dm1 netDp1 ../axtChainChrom/fruitflyFiltChrom.net
    # Move back to the file server to create axt files corresponding
    # to the net.
    ssh kksilo
    cd /cluster/data/dm1/bed/blastz.dp1.2003-11-14/axtChainChrom
    mkdir ../axtNetChrom
    netSplit fruitflyFiltChrom.net fruitflyNet
    cd fruitflyNet
    foreach i (*.net)
        set c = $i:r
        netToAxt -maxGap=300 $i ../chain/$c.chain /cluster/data/dm1/nib \
          /cluster/data/dp1/nib ../../axtNetChrom/$c.axt
        echo done ../axt/$c.axt
    end
    cd ..
    rm -r fruitflyNet
    cd /cluster/data/dm1/bed/blastz.dp1.2003-11-14/axtChainScaffoldQ
    mkdir ../axtNetScaffoldQ
    netSplit fruitflyFilt.net fruitflyNet
    cd fruitflyNet
    foreach i (*.net)
        set c = $i:r
        netToAxt -maxGap=300 $i ../chain/$c.chain /cluster/data/dm1/nib \
          /cluster/data/dp1/scaffoldsNib ../../axtNetScaffoldQ/$c.axt
        echo done ../axtNetScaffoldQ/$c.axt
    end
    cd ..
    rm -r fruitflyNet
    # Load up the axtNet (alignment score wiggle) track:
    ssh hgwdev
    mkdir /gbdb/dm1/axtNetDp1
    foreach f (/cluster/data/dm1/bed/blastz.dp1.2003-11-14/axtNetChrom/chr*.axt)
      ln -s $f /gbdb/dm1/axtNetDp1
    end
    hgLoadAxt dm1 axtNetDp1


PRODUCING GENSCAN PREDICTIONS (TODO 11/14/03 angie)
    # Run on small cluster -- genscan needs big mem.
    ssh kkr1u00
    mkdir /cluster/data/dm1/bed/genscan
    cd /cluster/data/dm1/bed/genscan
    # Make 3 subdirectories for genscan to put their output files in
    mkdir gtf pep subopt
    # Make hard-masked contigs
    foreach f (/cluster/data/dm1/?{,?}/chr*/chr?{,?}_?{,?}.fa)
      maskOutFa $f hard $f.masked
    end
    # Generate a list file, contigs.list, of all the hard-masked contigs that 
    # *do not* consist of all-N's (which would cause genscan to blow up)
    rm -f contigs.list
    touch contigs.list
    foreach f ( `ls -1S /cluster/data/dm1/?{,?}/chr*/chr?{,?}{,_random}_?{,?}.fa.masked` )
      egrep '[ACGT]' $f > /dev/null
      if ($status == 0) echo $f >> contigs.list
    end
    cat << '_EOF_' > gsub
#LOOP
/cluster/bin/i386/gsBig {check in line+ $(path1)} {check out line gtf/$(root1).gtf} -trans={check out line pep/$(root1).pep} -subopt={check out line subopt/$(root1).bed} -exe=/cluster/home/fanhsu/projects/compbio/bin/genscan-linux/genscan -par=/cluster/home/fanhsu/projects/compbio/bin/genscan-linux/HumanIso.smat -tmp=/tmp -window=2400000
#ENDLOOP
'_EOF_'
    # << this line keeps emacs coloring happy
    gensub2 contigs.list single gsub jobList
    para create jobList
    para try
    para check
    para push
#Completed: 79 of 79 jobs
#Average job time:                 136s       2.26m     0.04h    0.00d
#Longest job:                      229s       3.82m     0.06h    0.00d
#Submission to last job:          1718s      28.63m     0.48h    0.02d

    # If there are crashes, diagnose with "para problems".  
    # If a job crashes due to genscan running out of memory, re-run it 
    # manually with "-window=1200000" instead of "-window=2400000".
    # chr14_21, chr16_4
    
    # Convert these to chromosome level files as so:
    ssh kksilo
    cd /cluster/data/dm1/bed/genscan
    liftUp genscan.gtf ../../jkStuff/liftAll.lft warn gtf/*.gtf
    liftUp genscanSubopt.bed ../../jkStuff/liftAll.lft warn subopt/*.bed
    cat pep/*.pep > genscan.pep

    # Load into the database as so:
    ssh hgwdev
    cd /cluster/data/dm1/bed/genscan
    ldHgGene dm1 genscan genscan.gtf
    hgPepPred dm1 generic genscanPep genscan.pep
    hgLoadBed dm1 genscanSubopt genscanSubopt.bed


# EXTEND BDGPGENE AND CREATE BDGPNEAR FOR HGNEAR (REDONE 6/18/04 angie)
    ssh hgwdev
    cd /cluster/data/dm1/bed/bdgpAnnotations.3.2
    cp ~/kent/src/hg/lib/bdgpSwissProt.sql .
    perl -we '%bName2all = (); \
              %bName2fb = (); \
              open(P, "echo \"select * from bdgpGeneInfo\" | hgsql -N dm1|") \
                || die; \
              while (<P>) { \
                chop; my @words = split("\t"); \
                $bName2fb{$words[0]} = \@words; \
              } \
              close(P); \
              open(P, "echo \"select bdgpGeneInfo.*,flyBaseSwissProt.* from bdgpGeneInfo,flyBaseSwissProt where bdgpGeneInfo.flyBaseId = flyBaseSwissProt.flyBaseId\" | hgsql -N dm1|") || die; \
              while (<P>) { \
                chop; my @words = split("\t"); \
                $bName2all{$words[0]} = \@words; \
              } \
              close(P); \
              open(P, "echo select name from bdgpGene | hgsql -N dm1|") ||die; \
              while (<P>) { \
                chop; $name = $_; \
                $bName = $name;  $bName =~ s/-R.*//; \
                if (exists($bName2all{$bName})) { \
                  ($bName, $fbID, $go, $symbol, $cyto, undef, \
                   undef, $spID, $spDesc, $spSymb) = @{$bName2all{$bName}}; \
                  print "INSERT INTO bdgpSwissProt VALUES ( \"$name\", \"$fbID\", \"$go\", \"$symbol\", \"$cyto\", \"$spID\", \"$spDesc\", \"$spSymb\");\n"; \
                } elsif (exists($bName2fb{$bName})) { \
                  ($bName, $fbID, $go, $symbol, $cyto, undef) = @{$bName2fb{$bName}}; \
                  $go = "" if (not defined $go); \
                  $cyto = "" if (not defined $cyto); \
                  print "INSERT INTO bdgpSwissProt VALUES ( \"$name\", \"$fbID\", \"$go\", \"$symbol\", \"$cyto\", \"n/a\", \"n/a\", \"n/a\");\n"; \
                } else { die "No info for $name."; } \
              } \
              close(P); ' \
    >> bdgpSwissProt.sql
    hgsql dm1 < bdgpSwissProt.sql

    # Use the above table to add a proteinID field to bdgpGene.  
    hgsql dm1 -e 'create table bdgpGene2 \
                  select bdgpGene.*, bdgpSwissProt.swissProtId as proteinID \
                  from bdgpGene, bdgpSwissProt \
                  where bdgpGene.name = bdgpSwissProt.bdgpName'

    # Now examine bdgpGene2 vs. bdgpGene manually, carefully.  
    # Do they have the same # rows?  
    hgsql dm1 -N -e 'select count(*) from bdgpGene'
    hgsql dm1 -N -e 'select count(*) from bdgpGene2'
    # Are most proteinID fields non-"n/a"? 
    hgsql dm1 -N -e 'select count(*) from bdgpGene2 where proteinID = "n/a"'
    # Spot-check some genes... are the fields of bdgpGene2 identical to 
    # the fields of bdgpGene, except for the new proteinIDs?  
    hgsql dm1 -N -e 'select * from bdgpGene limit 3'
    hgsql dm1 -N -e 'select * from bdgpGene2 limit 3'
    # If so, then go ahead:
    hgsql dm1 -e 'drop table bdgpGene; rename table bdgpGene2 to bdgpGene;'
    # and check the new bdgpGene manually.


#############################################################################
# MAKE HGNEAR
# adapted from makeHgNear.doc; split into sections.  See makeHgFixed.doc 
# for how Arbeitman et al's fly lifecycle expression data were loaded into 
# hgFixed.  

# BLASTP SELF, CLUSTER GENES, MAP TO EXP.DATA FOR HGNEAR (REDONE 6/18/04 angie)
    ssh hgwdev
    # Now that bdgpGene has proteinID, use hgClusterGenes to cluster
    # together various alt-splicing isoforms, creating the tables
    # bdgpIsoforms and bdgpCanonical.  
    hgClusterGenes dm1 bdgpGene bdgpIsoforms bdgpCanonical
    # Extract peptides from bdgpGenes into fasta file
    # and create a blast database out of them.
    mkdir /cluster/data/dm1/bed/blastp
    cd /cluster/data/dm1/bed/blastp
    pepPredToFa dm1 bdgpGenePep bdgp.faa
    formatdb -i bdgp.faa -t bdgp -n bdgp

    # Copy over database to iscratch/i
    ssh kkr1u00
    if (-e /iscratch/i/dm1/blastp) then
      rm -r /iscratch/i/dm1/blastp
    endif
    mkdir -p /iscratch/i/dm1/blastp
    cp /cluster/data/dm1/bed/blastp/bdgp.* /iscratch/i/dm1/blastp

    # Load up iscratch/i with blastp and related files
    # if necessary
    if (! -e /iscratch/i/blast/blastall) then
      mkdir -p /iscratch/i/blast
      cp /projects/compbio/bin/i686/blastall /iscratch/i/blast
      mkdir -p /iscratch/i/blast/data
      cp /projects/compbio/bin/i686/data/* /iscratch/i/blast/data
    endif
    iSync

    # Split up fasta file into bite sized chunks for cluster
    ssh kksilo
    cd /cluster/data/dm1/bed/blastp
    mkdir split
    faSplit sequence bdgp.faa 6000 split/bg

    # Make parasol run directory 
    ssh kk
    mkdir -p /cluster/data/dm1/bed/blastp/self/run/out
    cd /cluster/data/dm1/bed/blastp/self/run
    # Make blast script
    cat > blastSome <<end
#!/bin/csh
setenv BLASTMAT /iscratch/i/blast/data
/iscratch/i/blast/blastall -p blastp -d /iscratch/i/dm1/blastp/bdgp -i \$1 -o \$2 -e 0.01 -m 8 -b 1000
end
    chmod a+x blastSome
    # Make gensub2 file
    cat > gsub <<end
#LOOP
blastSome {check in line+ \$(path1)} {check out line out/\$(root1).tab}
#ENDLOOP
end
    # Create parasol batch
    ls -1S ../../split/*.fa > split.lst
    gensub2 split.lst single gsub spec
    para create spec
    para try, check, push, check, ...
#Completed: 5821 of 5821 jobs
#Average job time:                  10s       0.16m     0.00h    0.00d
#Longest job:                      326s       5.43m     0.09h    0.00d
#Submission to last job:           655s      10.92m     0.18h    0.01d

    # Load into database.  This took only ~3 minutes for dm1.
    ssh hgwdev
    cd /cluster/data/dm1/bed/blastp/self/run/out
    time hgLoadBlastTab dm1 bdgpBlastTab *.tab
    # Create table that maps between bdgp genes and RefSeq
    hgMapToGene dm1 refGene bdgpGene bdgpToRefSeq
    # Create table that maps between bdgp genes and LocusLink
    echo "select mrnaAcc,locusLinkId from refLink" | hgsql -N dm1 > refToLl.txt
### NOT DONE (LocusLink info temporarily missing from genbank)
    hgMapToGene dm1 refGene bdgpGene bdgpToLocusLink -lookup=refToLl.txt
###
    # Create table that maps between known genes and Pfam domains
    hgMapViaSwissProt dm1 bdgpGene name proteinID Pfam bdgpToPfam

    # Create a table that maps BDGP root names to canonical transcripts:
    cd /cluster/data/dm1/bed/blastp
    cat > bdgpToCanonical.sql <<end
CREATE TABLE bdgpToCanonical (
  name varchar(255) NOT NULL default '',
  value varchar(255) NOT NULL default '',
  KEY name (name(16)),
  KEY value (value(16))
) TYPE=MyISAM;
end
    perl -we 'open(P, "echo select transcript from bdgpCanonical | hgsql dm1 -N |") || die; \
              while (<P>) { \
                chop; $name = $value = $_; $value =~ s/-R.$//; \
                print "INSERT INTO bdgpToCanonical VALUES (\"$name\", \"$value\");\n"; \
              } \
              close(P);' \
      >> bdgpToCanonical.sql
    hgsql dm1 < bdgpToCanonical.sql
    # Create a table that maps between bdgp genes and the 
    # Stanford Microarray Project expression data. (see makeHgFixed.doc)
    hgExpDistance -lookup=bdgpToCanonical \
      dm1 hgFixed.arbFlyLifeMedianRatio dummyArg arbExpDistance

    # Make sure that GO database is up to date.
    See README in /cluster/store1/geneOntology.


# C.ELEGANS BLASTP FOR HGNEAR (REDONE 6/17/04 angie)
    # Make C. elegans ortholog column using blastp on wormpep.
    # First make C. elegans protein database and copy it to iscratch/i
    # if it doesn't exist already:
    ssh eieio
    mkdir /cluster/data/ce2/bed/blastp
    cd /cluster/data/ce2/bed/blastp
    # Point a web browser at ftp://ftp.sanger.ac.uk/pub/databases/wormpep/
    # to find out the latest version.  Then use that in place of 126 below.
    wget -O wormPep126.faa \
      ftp://ftp.sanger.ac.uk/pub/databases/wormpep/wormpep126/wormpep
    formatdb -i wormPep126.faa -t wormPep126 -n wormPep126
    ssh kkr1u00
    if (-e /iscratch/i/ce2/blastp) then
      rm -r /iscratch/i/ce2/blastp
    endif
    mkdir -p /iscratch/i/ce2/blastp
    cp /cluster/data/ce2/bed/blastp/wormPep126.p?? /iscratch/i/ce2/blastp
    iSync

    # Make parasol run directory 
    ssh kk
    mkdir -p /cluster/data/dm1/bed/blastp/ce2/run/out
    cd /cluster/data/dm1/bed/blastp/ce2/run
    # Make blast script
    cat > blastSome <<end
#!/bin/csh
setenv BLASTMAT /iscratch/i/blast/data
/iscratch/i/blast/blastall -p blastp -d /iscratch/i/ce2/blastp/wormPep126 -i \$1 -o \$2 -e 0.01 -m 8 -b 1
end
    chmod a+x blastSome
    # Make gensub2 file
    cat > gsub <<end
#LOOP
blastSome {check in line+ \$(path1)} {check out line out/\$(root1).tab}
#ENDLOOP
end
    # Create parasol batch
    ls -1S ../../split/*.fa > split.lst
    gensub2 split.lst single gsub spec
    para create spec
    para try, check, push, check, ...
#Completed: 5821 of 5821 jobs
#Average job time:                   8s       0.14m     0.00h    0.00d
#Longest job:                      156s       2.60m     0.04h    0.00d
#Submission to last job:           156s       2.60m     0.04h    0.00d

    # Load into database.  
    ssh hgwdev
    cd /cluster/data/dm1/bed/blastp/ce2/run/out
    hgLoadBlastTab dm1 ceBlastTab -maxPer=1 *.tab


# MOUSE BLASTP FOR HGNEAR (REDONE 6/17/04 angie)
    # Make mouse ortholog column using blastp on mouse known genes.
    # First make mouse protein database and copy it to iscratch/i
    # if it doesn't exist already:
    ssh hgwdev
    mkdir /cluster/data/mm4/bed/blastp
    cd /cluster/data/mm4/bed/blastp
    pepPredToFa mm4 knownGenePep known.faa
    formatdb -i known.faa -t known -n known
    ssh kkr1u00
    if (-e /iscratch/i/mm4/blastp) then
      rm -r /iscratch/i/mm4/blastp
    endif
    mkdir -p /iscratch/i/mm4/blastp
    cp -p /cluster/data/mm4/bed/blastp/known.p?? /iscratch/i/mm4/blastp
    iSync

    # Make parasol run directory 
    ssh kk
    mkdir -p /cluster/data/dm1/bed/blastp/mm4/run/out
    cd /cluster/data/dm1/bed/blastp/mm4/run
    # Make blast script
    cat > blastSome <<end
#!/bin/csh
setenv BLASTMAT /iscratch/i/blast/data
/iscratch/i/blast/blastall -p blastp -d /iscratch/i/mm4/blastp/known -i \$1 -o \$2 -e 0.001 -m 8 -b 1
end
    chmod a+x blastSome
    # Make gensub2 file
    cat > gsub <<end
#LOOP
blastSome {check in line+ \$(path1)} {check out line out/\$(root1).tab}
#ENDLOOP
end
    # Create parasol batch
    ls -1S ../../split/*.fa > split.lst
    gensub2 split.lst single gsub spec
    para create spec
    para try, check, push, check, ...
#Completed: 5821 of 5821 jobs
#Average job time:                  12s       0.20m     0.00h    0.00d
#Longest job:                      267s       4.45m     0.07h    0.00d
#Submission to last job:           267s       4.45m     0.07h    0.00d

    # Load into database.  
    ssh hgwdev
    cd /cluster/data/dm1/bed/blastp/mm4/run/out
    hgLoadBlastTab dm1 mmBlastTab -maxPer=1 *.tab


# HUMAN BLASTP FOR HGNEAR (REDONE 6/17/04 angie)
    # Make human ortholog column using blastp on human known genes.
    # First make human protein database and copy it to iscratch/i
    # if it doesn't exist already:
    mkdir /cluster/data/hg16/bed/blastp
    cd /cluster/data/hg16/bed/blastp
    pepPredToFa hg16 knownGenePep known.faa
    formatdb -i known.faa -t known -n known
    ssh kkr1u00
    if (-e /iscratch/i/hg16/blastp) then
      rm -r /iscratch/i/hg16/blastp
    endif
    mkdir -p /iscratch/i/hg16/blastp
    cp /cluster/data/hg16/bed/blastp/known.p?? /iscratch/i/hg16/blastp
    iSync

    # Make parasol run directory 
    ssh kk
    mkdir -p /cluster/data/dm1/bed/blastp/hg16/run/out
    cd /cluster/data/dm1/bed/blastp/hg16/run
    # Make blast script
    cat > blastSome <<end
#!/bin/csh
setenv BLASTMAT /iscratch/i/blast/data
/iscratch/i/blast/blastall -p blastp -d /iscratch/i/hg16/blastp/known -i \$1 -o \$2 -e 0.001 -m 8 -b 1
end
    chmod a+x blastSome
    # Make gensub2 file
    cat > gsub <<end
#LOOP
blastSome {check in line+ \$(path1)} {check out line out/\$(root1).tab}
#ENDLOOP
end
    # Create parasol batch
    ls -1S ../../split/*.fa > split.lst
    gensub2 split.lst single gsub spec
    para create spec
    para try, check, push, check, ...
#Completed: 5821 of 5821 jobs
#Average job time:                  14s       0.24m     0.00h    0.00d
#Longest job:                      327s       5.45m     0.09h    0.00d
#Submission to last job:           327s       5.45m     0.09h    0.00d

    # Load into database.  
    ssh hgwdev
    cd /cluster/data/dm1/bed/blastp/hg16/run/out
    hgLoadBlastTab dm1 hgBlastTab -maxPer=1 *.tab


# ZEBRAFISH BLASTP FOR HGNEAR (REDONE 6/17/04 angie)
    # Make Danio rerio (zebrafish) ortholog column using blastp on Ensembl.
    # First make protein database and copy it to iscratch/i
    # if it doesn't exist already:
    ssh kkstore
    mkdir /cluster/data/danRer1/bed/blastp
    cd /cluster/data/danRer1/bed/blastp
    wget ftp://ftp.ensembl.org/pub/current_zebrafish/data/fasta/pep/Danio_rerio.ZFISH3.may.pep.fa.gz 
    zcat Dan*.pep.fa.gz > ensembl.faa
    formatdb -i ensembl.faa -t ensembl -n ensembl
    ssh kkr1u00
    if (-e /iscratch/i/danRer1/blastp) then
      rm -r /iscratch/i/danRer1/blastp
    endif
    mkdir -p /iscratch/i/danRer1/blastp
    cp /cluster/data/danRer1/bed/blastp/ensembl.p?? /iscratch/i/danRer1/blastp
    iSync

    # Make parasol run directory 
    ssh kk
    mkdir -p /cluster/data/dm1/bed/blastp/danRer1/run/out
    cd /cluster/data/dm1/bed/blastp/danRer1/run
    # Make blast script
    cat > blastSome <<end
#!/bin/csh
setenv BLASTMAT /iscratch/i/blast/data
/iscratch/i/blast/blastall -p blastp -d /iscratch/i/danRer1/blastp/ensembl -i \$1 -o \$2 -e 0.005 -m 8 -b 1
end
    chmod a+x blastSome
    # Make gensub2 file
    cat > gsub <<end
#LOOP
blastSome {check in line+ \$(path1)} {check out line out/\$(root1).tab}
#ENDLOOP
end
    # Create parasol batch
    ls -1S ../../split/*.fa > split.lst
    gensub2 split.lst single gsub spec
    para create spec
    para try, check, push, check, ...
#Completed: 5821 of 5821 jobs
#Average job time:                  12s       0.20m     0.00h    0.00d
#Longest job:                      273s       4.55m     0.08h    0.00d
#Submission to last job:           273s       4.55m     0.08h    0.00d

    # Load into database.  
    ssh hgwdev
    cd /cluster/data/dm1/bed/blastp/danRer1/run/out
    hgLoadBlastTab dm1 drBlastTab -maxPer=1 *.tab


# YEAST BLASTP FOR HGNEAR (REDONE 6/17/04 angie)
    # Make Saccharomyces cerevisiae (yeast) ortholog column using blastp on 
    # RefSeq.  First make protein database and copy it to iscratch/i
    # if it doesn't exist already:
    mkdir /cluster/data/sacCer1/bed/blastp
    cd /cluster/data/sacCer1/bed/blastp
    wget ftp://genome-ftp.stanford.edu/pub/yeast/data_download/sequence/genomic_sequence/orf_protein/orf_trans.fasta.gz
    zcat orf_trans.fasta.gz > sgdPep.faa
    formatdb -i sgdPep.faa -t sgdPep -n sgdPep
    ssh kkr1u00
    # Note: sacCer1 is a name conflict with SARS coronavirus... oh well, 
    # fortunately we won't be looking for homologs there.  :)
    if (-e /iscratch/i/sacCer1/blastp) then
      rm -r /iscratch/i/sacCer1/blastp
    endif
    mkdir -p /iscratch/i/sacCer1/blastp
    cp /cluster/data/sacCer1/bed/blastp/sgdPep.p?? /iscratch/i/sacCer1/blastp
    iSync

    # Make parasol run directory 
    ssh kk
    mkdir -p /cluster/data/dm1/bed/blastp/sacCer1/run/out
    cd /cluster/data/dm1/bed/blastp/sacCer1/run
    # Make blast script
    cat > blastSome <<end
#!/bin/csh
setenv BLASTMAT /iscratch/i/blast/data
/iscratch/i/blast/blastall -p blastp -d /iscratch/i/sacCer1/blastp/sgdPep -i \$1 -o \$2 -e 0.01 -m 8 -b 1
end
    chmod a+x blastSome
    # Make gensub2 file
    cat > gsub <<end
#LOOP
blastSome {check in line+ \$(path1)} {check out line out/\$(root1).tab}
#ENDLOOP
end
    # Create parasol batch
    ls -1S ../../split/*.fa > split.lst
    gensub2 split.lst single gsub spec
    para create spec
    para try, check, push, check, ...
#Completed: 5821 of 5821 jobs
#Average job time:                   4s       0.07m     0.00h    0.00d
#Longest job:                       37s       0.62m     0.01h    0.00d
#Submission to last job:            42s       0.70m     0.01h    0.00d

    # Load into database.  
    ssh hgwdev
    cd /cluster/data/dm1/bed/blastp/sacCer1/run/out
    hgLoadBlastTab dm1 scBlastTab -maxPer=1 *.tab


# MAKE ORGANISM-SPECIFIC HGNEARDATA FILES (DONE 10/17/03 angie)
    cd ~/kent/src/hg/near/hgNear/hgNearData
    # The directory name is the dbDb.genome field, processed by 
    # hdb.c's hgDirForOrg():
    mkdir D_melanogaster
    cp C_elegans/*.{html,ra} D_melanogaster/
    cd D_melanogaster
    mv kimLifeCycleFull.html arbLifeCycleFull.html
    mv kimLifeCycleMedian.html arbLifeCycleMedian.html
    # edit all .ra and .html files as appropriate for D. melanogaster/dm1...
    # cvs add and check in D_melanogaster/ and all .ra and .html files.
    # The "representatives" lines in columnDb.ra are tricky.  
    # For the median reps, I used this, edited to include an extra "-1," 
    # at each boundary between evelopmental stages (embryo -> larva, etc):
    # 29->30, 41->42, 59->60
    echo "select id from arbFlyLifeMedianExps;" | hgsql -N hgFixed \
    | perl -we '$i=0; \
                while (<>) { \
                  chop; print "$_,"; \
                  $i++; if (($i % 5) == 0) { print "-1," }; \
                } \
                print "\n";'
    # For the full reps, I used the output of this command minus the 
    # initial ",-1,".  -1 separators are inserted between each group 
    # that was lumped together in arbMed.ra:
    awk '-F      ' '{print $3;}' \
      ~/kent/src/hg/makeDb/hgMedianMicroarray/arbMed.ra \
    | perl -we 'while (<>) { \
                  @w=split(" "); \
                   print ",-1," . join(",", @w); \
                } \
                print "\n";'
    # Figure out what values to use for expMulti defn's absoluteMax.
    # Report on the max and 99.5%ile absolute score:
    echo select expScores from arbFlyLifeAll | hgsql hgFixed -N \
    | perl -we '$max = 0.0; \
                @all = (); \
                while (<>) { \
                  chop; @nums = split(","); \
                  foreach $num (@nums) { \
                    $max = $num if ($num > $max); \
                    push @all, $num if (defined $num && $num ne ""); \
                  } \
                } \
                $n = scalar(@all); \
                print "max is $max, N is $n\n"; \
                @all = sort { $a <=> $b } @all; \
                $useMax = $all[$n * 0.995 + 1]; \
                print "99.5%ile is $useMax -- use this for max\n";'
# max is 65535.000, N is 797364
# 99.5%ile is 15433.000 -- use this for max
    # N is the product of count(*) of arbFlyLifeAll{,Exps} .  
    # Now repeat the above perl in-liner, but on arbFlyLifeAllRatio, 
    # to determine ratioMax:  
    echo select expScores from arbFlyLifeAllRatio | hgsql hgFixed -N \
    # <paste in perl from above>
# max is 9.862, N is 797364
# 99.5%ile is 2.915 -- use this for max


# ENABLE HGNEAR FOR DM1 IN HGCENTRALTEST (DONE 10/17/03 angie)
    echo "update dbDb set hgNearOk = 1 where name = 'dm1';" \
      | hgsql -h genome-testdb hgcentraltest


# END OF HGNEAR STUFF
#############################################################################


# reload refseqs to pickup a change that uses locus_tag if gene name isn't
# available.  2003/10/16 markd

# First remove refseqs from databases:
    drop table refSeqStatus;
    drop table refLink;
    drop table refSeqSummary;
    drop table refGene;
    drop table refSeqAli;
    drop table refFlat;

    delete from gbSeq where srcDb = 'RefSeq';
    delete from gbStatus where srcDb = 'RefSeq';
    delete from gbExtFile where path like '%/refseq%';
    delete from gbLoaded where srcDb = 'RefSeq';
    delete from mrna where acc like 'NM\_%';
    delete from imageClone where acc like 'NM\_%';
    delete from mrna where acc like 'NR\_%';
    delete from imageClone where acc like 'NR\_%';

# now reload:
    cd /cluster/data/genbank
    ./bin/gbDbLoadStep -type=mrna dm1


# MAKE DOWNLOADABLE FILES (DONE 10/29/03 angie)
    ssh kksilo
    cd /cluster/data/dm1
    mkdir zips
    zip -j zips/chromOut.zip ?{,?}/chr?{,?}.fa.out
    zip -j zips/chromFa.zip ?{,?}/chr?{,?}.fa
    foreach f (?{,?}/chr?{,?}.fa)
      maskOutFa $f hard $f.masked
    end
    zip -j zips/chromFaMasked.zip ?{,?}/chr?{,?}.fa.masked
    cd bed/simpleRepeat
    zip ../../zips/chromTrf.zip trfMaskChrom/chr*.bed
    zip ../../zips/contigTrf.zip trfMask/N{T,G}*.bed
    cd ../..
    # Make a starter mrna.zip -- it will get updated regularly on the RR. 
    /cluster/data/genbank/bin/i386/gbGetSeqs -gbRoot=/cluster/data/genbank \
      -db=dm1 -native genbank mrna mrna.fa
    zip zips/mrna.zip mrna.fa
    rm mrna.fa
    foreach f (zips/*.zip)
      echo $f
      unzip -t $f | tail -1
    end
    ssh hgwdev
    mkdir /usr/local/apache/htdocs/goldenPath/dm1
    cd /usr/local/apache/htdocs/goldenPath/dm1
    mkdir bigZips database
    # Create README.txt files in bigZips/ and database/ to explain the files.
    cp -p /cluster/data/dm1/zips/*.zip bigZips


# ADD BLASTZ/CHAIN/NET DOWNLOADABLE FILES (DONE 12/1/03 angie)
    ssh hgwdev
    mkdir /usr/local/apache/htdocs/goldenPath/dm1/vsDp1
    cd /usr/local/apache/htdocs/goldenPath/dm1/vsDp1
    gzip -c \
      /cluster/data/dm1/bed/blastz.dp1.2003-11-14/axtChainScaffoldQ/all.chain \
      > chain.gz
    gzip -c \
      /cluster/data/dm1/bed/blastz.dp1.2003-11-14/axtChainScaffoldQ/fruitflyFilt.net \
      > net.gz
    mkdir axtNet
    foreach f (/cluster/data/dm1/bed/blastz.dp1.2003-11-14/axtNetScaffoldQ/chr*axt)
      gzip -c $f > axtNet/$f:t.gz
    end
    # Make a README.txt which explains the files & formats.


# ANOPHELES ECORES FROM GENOSCOPE (DONE 11/10/03 angie)
    ssh hgwdev
    mkdir /cluster/data/dm1/bed/anophelesEcores
    cd /cluster/data/dm1/bed/anophelesEcores
    # save attachment from Olivier Jaillon's email 11/10/03 to 
    # ecotig.6.4ucsc
    perl -wpe 'if (/^(\w+)\:\d+ (\d+) (\d+) (\w+)\:\d+ (\d+) (\d+)$/) { \
                 if ($1 ne $4) { die "diff chr: $1 $4"; } \
                 $name = "chr$1:$2-$6";  $start = $2 - 1; \
                 $sz1 = $3 - $start;  $sz2 = $6 - ($5 - 1);   $st2 = $5 - $2; \
                 $_ = "chr$1\t$start\t$6\t$name\t0\t+\t$start\t$6\t0\t2\t$sz1,$sz2,\t0,$st2,\n"; \
               } elsif (/^(\w+)\:\d+ (\d+) (\d+)\s*$/) { \
                 $name = "chr$1:$2-$3";  $start = $2 - 1;  $sz1 = $3 - $start; \
                 $_ = "chr$1\t$start\t$3\t$name\t0\t+\t$start\t$3\t0\t1\t$sz1,\t0,\n"; \
               } else { chop; die "cant parse line $.:\n|$_|"; }' \
    < ecotig.6.4ucsc > anophelesEcores.bed
    hgLoadBed -tab dm1 anophelesEcores anophelesEcores.bed


# MASKED-QUERY XENO RNA ALIGNMENTS (DONE 10/22/03 angie)
    # Experimental track... if this TRF-masking works out, then find out 
    # what it would take to add it as an option to Mark's genbank stuff.
    ssh kksilo
    mkdir /cluster/data/dm1/bed/xenoMrnaMasked
    cd /cluster/data/dm1/bed/xenoMrnaMasked
    # Grab the latest full-release (137) genbank mRNA's.  Strip the 
    # .version suffixes so we can use accessions from mrna.gbidx to 
    # pick out the non-D.mel. mrnas.  
    perl -wpe 's/^>(\w+).\d+/>$1/' \
      /cluster/data/genbank/data/processed/genbank.137.0/full/mrna.fa \
      > allMrna.fa
    grep -v "Drosophila melanogaster" \
      /cluster/data/genbank/data/processed/genbank.137.0/full/mrna.gbidx \
        | awk '{print $1;}' | grep -v "^#" \
    > xenoAcc.lst
    faSomeRecords allMrna.fa xenoAcc.lst flyXenoRna.fa
    # Split up the sequences into manageably sized files.  
    mkdir flyXenoRnaSplit
    faSplit about flyXenoRna.fa 10000000 flyXenoRnaSplit/xenoRna
    # Now TRF-mask the sequences... use repeats with period <= 9 to mask.
    mkdir trf
    rm -f trf.log; touch trf.log
    foreach f (flyXenoRnaSplit/*.fa)
      set b = trf/$f:t:r.bed
      echo $f to $b...
      /cluster/bin/i386/trfBig -trf=/cluster/bin/i386/trf $f /dev/null \
        -bedAt=$b -tempDir=/tmp -maxPeriod=9 >& trf.log
      maskOutFa -soft $f $b $f
    end
    # distribute masked xenoRna sequence on i-servers
    mkdir /cluster/bluearc/dm1/mrna.137
    cp -p /cluster/data/dm1/bed/xenoMrnaMasked/flyXenoRnaSplit/*.fa \
      /cluster/bluearc/dm1/mrna.137/

    ssh kk
    cd /cluster/data/dm1/bed/xenoMrnaMasked
    mkdir psl
    ls -1S /iscratch/i/dm1/nib/chr*.nib > fly.lst
    ls -1S /cluster/bluearc/dm1/mrna.137/xenoRna*.fa > mrna.lst
    echo '#LOOP \
/cluster/bin/i386/blat -maxIntron=50000 $(path1) {check in line+ $(path2)} -q=rnax -t=dnax -mask=lower {check out line+ psl/$(root1)_$(root2).psl} \
#ENDLOOP' > gsub
    gensub2 fly.lst mrna.lst gsub spec
    para create spec
    para try, check, push, check, ...
#Completed: 814 of 814 jobs
#Average job time:                 459s       7.65m     0.13h    0.01d
#Longest job:                     1756s      29.27m     0.49h    0.02d
#Submission to last job:          2145s      35.75m     0.60h    0.02d
    ssh kksilo
    cd /cluster/data/dm1/bed/xenoMrnaMasked
    pslSort dirs raw.psl /cluster/store2/temp psl
    pslReps raw.psl cooked.psl /dev/null -minAli=0.25
    # pslFilter -minMatch=60 -gapSizeLogMod=2 -minScore=30 cooked.psl filt.psl
    pslFilter -minAli=250 -minUniqueMatch=15 cooked.psl filt.psl
    pslSortAcc nohead chrom /cluster/store2/temp filt.psl
    pslCat -dir chrom > xenoMrnaMasked.psl
    rm -r chrom raw.psl cooked.psl filt.psl
    # Load into database as so:
    ssh hgwdev
    cd /cluster/data/dm1/bed/xenoMrnaMasked
    hgLoadPsl dm1 xenoMrnaMasked.psl
    # Looks like no need to hgLoadSeq -- seqs are already loaded by Mark's 
    # stuff.


# BLAT HONEYBEE (apiMel0) (DONE 1/13/04 angie)
    ssh kk
    mkdir /cluster/data/dm1/bed/blatApiMel0
    cd /cluster/data/dm1/bed/blatApiMel0
    mkdir psl
    ls -1S /iscratch/i/dm1/nib/*.nib > fly.lst
    ls -1S /iscratch/i/apiMel0/chunks/*.fa > bee.lst
cat << 'EOF' > gsub
#LOOP
/cluster/bin/i386/blat -mask=lower -qMask=lower -q=dnax -t=dnax {check in exists+ $(path1)} {check in line+ $(path2)} {check out line+ /cluster/data/dm1/bed/blatApiMel0/psl/$(root1)_$(root2).psl}
#ENDLOOP
'EOF'
    # << this line makes emacs coloring happy
    gensub2 fly.lst bee.lst gsub spec
    para create spec
    para try, check, push, check, ...
#Completed: 495 of 495 jobs
#Average job time:                 802s      13.37m     0.22h    0.01d
#Longest job:                    26590s     443.17m     7.39h    0.31d
#Submission to last job:         27028s     450.47m     7.51h    0.31d
    # postprocess
    pslCat -dir psl > blatApiMel0.psl
    # load
    ssh hgwdev
    cd /cluster/data/dm1/bed/blatApiMel0
    hgLoadPsl dm1 blatApiMel0.psl
    mkdir /gbdb/dm1/apiMel0
    foreach f (/cluster/data/apiMel0/groups/*.fa)
      ln -s $f /gbdb/dm1/apiMel0/$f:t
    end
    hgLoadSeq dm1 /gbdb/dm1/apiMel0/*.fa


#  miRNA track (DONE - 2004-05-04 - Hiram)
    #	data from: Sam Griffiths-Jones <sgj@sanger.ac.uk>
    #	and Michel.Weber@ibcg.biotoul.fr
    #	notify them if this assembly updates to renew this track
    ssh hgwdev
    mkdir /cluster/data/dm1/bed/miRNA
    cd /cluster/data/dm1/bed/miRNA
    wget --timestamping \
    "ftp://ftp.sanger.ac.uk/pub/databases/Rfam/miRNA/genomes/dme_bdgp3.*"
    grep -v "^track " dme_bdgp3.bed | sed -e "s/ /\t/g" > dm1.bed
    hgLoadBed dm1 miRNA dm1.bed
    # entry in trackDb/trackDb.ra already there
    #	featureBits dm1 miRNA
    #	6845 bases of 126527731 (0.005%) in intersection


# BLASTZ D.YAKUBA (DONE 5/22/04 angie)
    ssh kksilo
    mkdir /cluster/data/dm1/bed/blastz.droYak1.2004-05-22
    cd /cluster/data/dm1/bed/blastz.droYak1.2004-05-22
    cat << '_EOF_' > DEF
# D.melanogaster vs. D.yakuba
export PATH=/usr/bin:/bin:/usr/local/bin:/cluster/bin/penn:/cluster/bin/i386:/cluster/home/angie/schwartzbin

ALIGN=blastz-run
BLASTZ=blastz
BLASTZ_H=2000
BLASTZ_ABRIDGE_REPEATS=0

# TARGET - D. melanogaster
SEQ1_DIR=/cluster/bluearc/drosophila/dm1/nib
# unused: SEQ1_RMSK=
SEQ1_SMSK=
SEQ1_FLAG=-drosophila
SEQ1_IN_CONTIGS=0
SEQ1_CHUNK=10000000
SEQ1_LAP=10000

# QUERY - D. yakuba
SEQ2_DIR=/iscratch/i/droYak1/nib
# unused: SEQ2_RMSK=
SEQ2_SMSK=
SEQ2_FLAG=-drosophila
SEQ2_IN_CONTIGS=0
SEQ2_CHUNK=10000000
SEQ2_LAP=0

BASE=/cluster/data/dm1/bed/blastz.droYak1.2004-05-22

DEF=$BASE/DEF
RAW=$BASE/raw
CDBDIR=$BASE
SEQ1_LEN=$BASE/S1.len
SEQ2_LEN=$BASE/S2.len

#DEBUG=1
'_EOF_'
    # << this line keeps emacs coloring happy
    # run bash shell if you don't already:
    bash
    source DEF
    mkdir run
    ~angie/hummus/make-joblist $DEF > $BASE/run/j
    sh ./xdir.sh
    cd run
    sed -e 's#^#/cluster/home/angie/schwartzbin/#' j > j2
    wc -l j*
    head j2
    mv j2 j
    # cluster run
    ssh kk
    cd /cluster/data/dm1/bed/blastz.droYak1.2004-05-22/run
    para create j
    para try, check, push, check, ....
#Completed: 672 of 672 jobs
#Average job time:                 164s       2.74m     0.05h    0.00d
#Longest job:                     1836s      30.60m     0.51h    0.02d
#Submission to last job:          2433s      40.55m     0.68h    0.03d

    # back on kksilo...
    mkdir /cluster/data/dm1/bed/blastz.droYak1.2004-05-22/run.1
    cd /cluster/data/dm1/bed/blastz.droYak1.2004-05-22/run.1
    ~angie/hummus/do.out2lav ../DEF > j
    # small cluster run
    ssh kki
    cd /cluster/data/dm1/bed/blastz.droYak1.2004-05-22/run.1
    para create j
    para try, check, push, check, ....
#Completed: 21 of 21 jobs
#Average job time:                 453s       7.54m     0.13h    0.01d
#Longest job:                      650s      10.83m     0.18h    0.01d
#Submission to last job:           685s      11.42m     0.19h    0.01d
    cd ..
    rm -r raw

    # third run: lav -> axt
    ssh kki
    cd /cluster/data/dm1/bed/blastz.droYak1.2004-05-22
    mkdir axtChrom pslChrom run.2
    cd run.2
    cat << '_EOF_' > do.csh
#!/bin/csh -ef
cd $1
set chr = $1:t
cat `ls -1 *.lav | sort -g` \
| $HOME/bin/x86_64/lavToAxt stdin \
    /cluster/bluearc/drosophila/dm1/nib /iscratch/i/droYak1/nib stdout \
| $HOME/bin/x86_64/axtSort stdin ../../axtChrom/$chr.axt 
$HOME/bin/x86_64/axtToPsl ../../axtChrom/$chr.axt ../../S1.len ../../S2.len \
  ../../pslChrom/$chr.psl
'_EOF_'
    # << this line keeps emacs coloring happy
    chmod a+x do.csh
    cp /dev/null jobList
    foreach d (../lav/chr*)
      echo "do.csh $d" >> jobList
    end
    para create jobList
    para try, check, push, check
#Completed: 11 of 11 jobs
#Average job time:                1516s      25.27m     0.42h    0.02d
#Longest job:                     2903s      48.38m     0.81h    0.03d
#Submission to last job:          2903s      48.38m     0.81h    0.03d

# CHAIN YAKUBA BLASTZ (DONE 5/27/04 angie)
    # Run axtChain on little cluster
    ssh kki
    cd /cluster/data/dm1/bed/blastz.droYak1.2004-05-22
    mkdir -p axtChain/run1
    cd axtChain/run1
    mkdir out chain
    ls -1S /cluster/data/dm1/bed/blastz.droYak1.2004-05-22/axtChrom/*.axt \
      > input.lst
    cat << '_EOF_' > gsub
#LOOP
doChain {check in exists $(path1)} {check out line+ chain/$(root1).chain} {check out exists out/$(root1).out}
#ENDLOOP
'_EOF_'
    # << this line makes emacs coloring happy

    cat << '_EOF_' > doChain
#!/bin/csh
axtFilter -notQ=chrUn_random $1 \
| axtChain -verbose=0 stdin \
  /cluster/bluearc/drosophila/dm1/nib \
  /iscratch/i/droYak1/nib $2 > $3
'_EOF_'
    # << this line makes emacs coloring happy
    chmod a+x doChain
    gensub2 input.lst single gsub jobList
    para create jobList
    para try, check, push, check...
#Completed: 11 of 11 jobs
#Average job time:                  18s       0.30m     0.01h    0.00d
#Longest job:                       35s       0.58m     0.01h    0.00d
#Submission to last job:            35s       0.58m     0.01h    0.00d

    # now on the cluster server, sort chains
    ssh kksilo
    cd /cluster/data/dm1/bed/blastz.droYak1.2004-05-22/axtChain
    chainMergeSort run1/chain/*.chain > all.chain
    chainSplit chain all.chain
    rm run1/chain/*.chain

    # take a look at score distr's
    foreach f (chain/*.chain)
      grep chain $f | awk '{print $2;}' | sort -nr > /tmp/score.$f:t:r
      echo $f:t:r
      textHistogram -binSize=10000 /tmp/score.$f:t:r
      echo ""
    end

    # Load chains into database
    ssh hgwdev
    cd /cluster/data/dm1/bed/blastz.droYak1.2004-05-22/axtChain/chain
    foreach i (*.chain)
        set c = $i:r
        echo loading $c
        hgLoadChain dm1 ${c}_chainDroYak1 $i
    end


# NET YAKUBA BLASTZ (DONE 5/27/04 angie)
    ssh kksilo
    cd /cluster/data/dm1/bed/blastz.droYak1.2004-05-22/axtChain
    chainPreNet all.chain ../S1.len ../S2.len stdout \
    | chainNet stdin -minSpace=1 ../S1.len ../S2.len stdout /dev/null \
    | netSyntenic stdin noClass.net

    # Add classification info using db tables:
    ssh hgwdev
    cd /cluster/data/dm1/bed/blastz.droYak1.2004-05-22/axtChain
    netClass -noAr noClass.net dm1 droYak1 yakuba.net

    # Make a 'syntenic' subset:
    ssh kksilo
    cd /cluster/data/dm1/bed/blastz.droYak1.2004-05-22/axtChain
    rm noClass.net
    netFilter -syn yakuba.net > yakubaSyn.net

    # Load the nets into database 
    ssh hgwdev
    cd /cluster/data/dm1/bed/blastz.droYak1.2004-05-22/axtChain
    netFilter -minGap=10 yakuba.net |  hgLoadNet dm1 netDroYak1 stdin
    netFilter -minGap=10 yakubaSyn.net | hgLoadNet dm1 netSyntenyDroYak1 stdin


# MAKE VSDROYAK1 DOWNLOADABLES (DONE 5/27/04 angie)
    ssh kksilo
    cd /cluster/data/dm1/bed/blastz.droYak1.2004-05-22/axtChain
    cp all.chain yakuba.chain
    zip /cluster/data/dm1/zips/yakuba.chain.zip yakuba.chain
    rm yakuba.chain
    zip /cluster/data/dm1/zips/yakuba.net.zip yakuba.net

    ssh hgwdev
    mkdir /usr/local/apache/htdocs/goldenPath/dm1/vsDroYak1
    cd /usr/local/apache/htdocs/goldenPath/dm1/vsDroYak1
    mv /cluster/data/dm1/zips/yakuba*.zip .
    md5sum *.zip > md5sum.txt
    # Copy over & edit README.txt w/pointers to chain, net formats.


# GENERATE DROYAK1 MAF FOR MULTIZ FROM NET (DONE 5/27/04 angie)
    ssh kksilo
    cd /cluster/data/dm1/bed/blastz.droYak1.2004-05-22/axtChain
    netSplit yakuba.net net
    ssh kolossus
    cd /cluster/data/dm1/bed/blastz.droYak1.2004-05-22
    mkdir axtNet
    foreach f (axtChain/net/*)
      set chr = $f:t:r
      netToAxt $f axtChain/chain/$chr.chain /cluster/data/dm1/nib \
        /cluster/data/droYak1/nib stdout \
      | axtSort stdin axtNet/$chr.axt
    end

    mkdir mafNet
    foreach f (axtNet/chr*.axt)
      set maf = mafNet/$f:t:r.my.maf
      axtToMaf $f \
            /cluster/data/dm1/chrom.sizes /cluster/data/droYak1/chrom.sizes \
            $maf -tPrefix=dm1. -qPrefix=droYak1.
    end


# GENERATE DP1 MAF FOR MULTIZ FROM NET (DONE 5/27/04 angie)
    ssh kolossus
    cd /cluster/data/dm1/bed/blastz.dp1.2003-11-14
    mkdir mafNet
    foreach f (axtNetChrom/chr*.axt)
      set maf = mafNet/$f:t:r.mp.maf
      axtSort $f stdout \
      | axtToMaf stdin \
            /cluster/data/dm1/chrom.sizes /cluster/data/dp1/chrom.sizes \
            $maf -tPrefix=dm1. -qPrefix=dp1.
    end


# MULTIZ MELANOGASTER/YAKUBA/PSEUDOOBSCURA (DONE 5/27/04 angie)
    # put the MAFs on bluearc
    ssh kksilo
    mkdir -p /cluster/bluearc/multiz.fly/my
    cp /cluster/data/dm1/bed/blastz.droYak1.2004-05-22/mafNet/*.maf \
      /cluster/bluearc/multiz.fly/my
    mkdir -p /cluster/bluearc/multiz.fly/mp
    cp /cluster/data/dm1/bed/blastz.dp1.2003-11-14/mafNet/*.maf \
      /cluster/bluearc/multiz.fly/mp


    ssh kki
    mkdir /cluster/data/dm1/bed/multiz.dm1droYak1dp1
    cd /cluster/data/dm1/bed/multiz.dm1droYak1dp1
    mkdir myp
    # Wrapper script required because of stdout redirect:
    cat << '_EOF_' > doMultiz
#!/bin/csh
/cluster/bin/penn/multiz $1 $2 - > $3
'_EOF_'
    # << this line makes emacs coloring happy
    chmod a+x doMultiz
    rm -f jobList
    foreach file (/cluster/bluearc/multiz.fly/my/*.maf) 
      set root=$file:t:r:r
      echo "doMultiz /cluster/bluearc/multiz.fly/mp/${root}.mp.maf $file /cluster/data/dm1/bed/multiz.dm1droYak1dp1/myp/${root}.maf" >> jobList
    end
    para create jobList
    para try, check, push, check
#Completed: 11 of 11 jobs
#Average job time:                  53s       0.88m     0.01h    0.00d
#Longest job:                      141s       2.35m     0.04h    0.00d
#Submission to last job:           141s       2.35m     0.04h    0.00d

    # clean up bluearc (these are big files!)
    rm -r /cluster/bluearc/multiz.fly

    # zip it for download
    zip -j /cluster/data/dm1/zips/multizDm1DroYak1Dp1.zip myp/*.maf

    # setup external files for database reference
    ssh hgwdev
    mkdir -p /gbdb/dm1/multizDroYak1Dp1
    ln -s /cluster/data/dm1/bed/multiz.dm1droYak1dp1/myp/*.maf \
      /gbdb/dm1/multizDroYak1Dp1
    # load into database
    hgLoadMaf -warn dm1 multizDroYak1Dp1
    # put it out for download
    mkdir /usr/local/apache/htdocs/goldenPath/dm1/multizDroYak1Dp1
    cd /usr/local/apache/htdocs/goldenPath/dm1/multizDroYak1Dp1
    mv /cluster/data/dm1/zips/multizDm1DroYak1Dp1.zip .
    md5sum *.zip > md5sum.txt


# BDGP 3.2 ANNOTATIONS (DONE 6/18/04 angie)
    ss kksilo
    cd /cluster/data/dm1/bed
    mv bdgpAnnotations bdgpAnnotations.3.1
    mkdir bdgpAnnotations.3.2
    cd /cluster/data/dm1/bed/bdgpAnnotations.3.2
    # No annotations for chrU or h's!  probably better that way...
    foreach c (2L 2R 3L 3R 4 X)
      set f = dmel_${c}_translation_r3.2.0.fasta.gz
      wget ftp://flybase.net/genomes/Drosophila_melanogaster/current/fasta/$f
      set f = dmel_${c}_r3.2.0.gff.gz
      wget ftp://flybase.net/genomes/Drosophila_melanogaster/current/gff/$f
    end
    gunzip dmel*.gff.gz
    # This new set is almost-but-not-quite GFF3, and all features (genes,
    # pseudogenes, ESTs, regulatory regions, kitchen sink) are included 
    # in one /gff file.  Perl the gene and non-coding gene parts of it into 
    # bdgp{Gene,NonCoding}.gtf and bdgp{Gene,NonCoding}Info.tab.
    chmod a+x ./extractGenes.pl
    ./extractGenes.pl *.gff
    # Sort GTF by position.
    sort -k1,1 -k4n,5n bdgpGene.gtf > tmp
    mv tmp bdgpGene.gtf
    sort -k1,1 -k4n,5n bdgpNonCoding.gtf > tmp
    mv tmp bdgpNonCoding.gtf

    # Proteins:
    zcat dmel*trans*.gz | perl -wpe 's/^>(CG\d+)(-\S+)?-P(\w)\s.*/>$1-R$3/' \
    > bdgpGenePep.fa

    # Load into test table for now -- don't want to mess up Gene Sorter 
    # stuff that depends on bdgpGene* tables.
    ssh hgwdev
    cd /cluster/data/dm1/bed/bdgpAnnotations.3.2
    ldHgGene -gtf -genePredExt dm1 bdgpGene2 bdgpGene.gtf
    featureBits dm1 bdgpGene
#28262131 bases of 126527731 (22.337%) in intersection
    featureBits dm1 bdgpGene2
#28178241 bases of 126527731 (22.270%) in intersection
    featureBits dm1 bdgpGene bdgpGene2
#27813359 bases of 126527731 (21.982%) in intersection
    # OK, looks OK.  Replace bdgpGene with bdgpGene2, reload bdgpGeneInfo,
    # then go back up and quickly rebuild all the hgNear (Gene Sorter) 
    # tables that depend on bdgpGene.  
    hgsql dm1 -e 'drop table bdgpGene; alter table bdgpGene2 rename bdgpGene'
    hgsql dm1 -e 'delete from bdgpGeneInfo'
    hgsql dm1 -e 'load data local infile "bdgpGeneInfo.tab" into table bdgpGeneInfo'
    # load bdgpNonCoding*
    ldHgGene -gtf -genePredExt dm1 bdgpNonCoding bdgpNonCoding.gtf
    hgsql dm1 -e 'delete from bdgpNonCodingInfo'
    hgsql dm1 -e 'load data local infile "bdgpNonCodingInfo.tab" into table bdgpNonCodingInfo'
    # load proteins
    hgPepPred dm1 generic bdgpGenePep bdgpGenePep.fa


# CYTOBANDS (DONE 6/15/04 angie)
    ssh hgwdev
    mkdir /cluster/data/dm1/bed/cytoband
    cd /cluster/data/dm1/bed/cytoband
    foreach c (2L 2R 3L 3R 4 X)
      wget ftp://flybase.net/genomes/Drosophila_melanogaster/current/gnomap/cytomap-$c.tsv
    end
    cp /dev/null cytoBand.tab
    foreach c (2L 2R 3L 3R 4 X)
      grep -v '^#' cytomap-$c.tsv \
      | awk '$3 > 0 && $2 != $3 {print "chr'$c'\t" $2 "\t" $3 "\t" $1 "\t";}' \
      >> cytoBand.tab
    end
    hgsql dm1 < ~/kent/src/hg/lib/cytoBand.sql
    hgsql dm1 -e 'load data local infile "cytoBand.tab" into table cytoBand'
    # Normally we would just create cytoBandIdeo by select * from cytoBand --
    # but that gives too high of a resolution for any bands to be visible!  
    # So make a boiled-down cytoBandIdeo that just has {number, letter} 
    # instead of {number, letter, number}.
    perl -we 'while (<>) { \
                chomp; @w=split(" "); \
                if ($w[3] =~ /(\d+[A-Z]+)\d+/) { \
                  $b = $1; \
                } else { die "doh!" } \
                if (! defined $lastB) { \
                  $start = $w[1]; \
                } elsif ($lastB ne $b) { \
                  print "$chrom\t$start\t$end\t$lastB\n"; \
                  $start = $w[1]; \
                } \
                ($chrom, $end, $lastB) = ($w[0], $w[2], $b); \
              } \
              print "$chrom\t$start\t$end\t$lastB\n";' cytoBand.tab \
     > cytoBandIdeo.tab
    hgsql dm1 -e 'create table cytoBandIdeo select * from cytoBand where 0'
    hgsql dm1 -e 'load data local infile "cytoBandIdeo.tab" into table \
                  cytoBandIdeo'
    # 6/23/04: trim the entries that fall off the ends of their respective 
    # chromosomes.
    foreach c (`echo select chrom from chromInfo | hgsql -N dm1`)
      set size = `echo select size from chromInfo where chrom = '"'$c'"' \
                  | hgsql -N dm1`
      echo $c $size
      foreach table (cytoBand cytoBandIdeo)
        echo delete from $table where chrom = \"$c\" and chromStart \> $size \
        | hgsql -N dm1
        echo update $table set chromEnd = $size where chrom = \"$c\" and \
             chromEnd \> $size \
        | hgsql -N dm1
      end
    end


# BLASTZ ANOPHELES (DONE 6/10/04 angie)
    # Will give human-fugu params a try... but without abridging repeats 
    # since I don't know which are lin-spec for fly vs. mosquito, and don't 
    # want to bother Arian or speculate.
    ssh kksilo
    mkdir /cluster/data/dm1/bed/blastz.anoGam1.2004-06-10
    cd /cluster/data/dm1/bed/blastz.anoGam1.2004-06-10
    cat << '_EOF_' > DEF
# D.melanogaster vs. A. gambiae
export PATH=/usr/bin:/bin:/usr/local/bin:/cluster/bin/penn:/cluster/bin/i386:/cluster/home/angie/schwartzbin

ALIGN=blastz-run
BLASTZ=blastz
BLASTZ_H=2000
BLASTZ_Y=3400
BLASTZ_L=6000
BLASTZ_K=2200
BLASTZ_Q=/cluster/data/blastz/HoxD55.q
BLASTZ_ABRIDGE_REPEATS=0

# TARGET - D. melanogaster
SEQ1_DIR=/cluster/bluearc/drosophila/dm1/nib
# unused: SEQ1_RMSK=
SEQ1_SMSK=
SEQ1_FLAG=-drosophila
SEQ1_IN_CONTIGS=0
SEQ1_CHUNK=10000000
SEQ1_LAP=10000

# QUERY - A. gambiae
SEQ2_DIR=/cluster/bluearc/anoGam1/nib
# unused: SEQ2_RMSK=
SEQ2_SMSK=
SEQ2_FLAG=-anopheles
SEQ2_IN_CONTIGS=0
SEQ2_CHUNK=10000000
SEQ2_LAP=0

BASE=/cluster/data/dm1/bed/blastz.anoGam1.2004-06-10

DEF=$BASE/DEF
RAW=$BASE/raw
CDBDIR=$BASE
SEQ1_LEN=$BASE/S1.len
SEQ2_LEN=$BASE/S2.len

#DEBUG=1
'_EOF_'
    # << this line keeps emacs coloring happy
    # run bash shell if you don't already:
    bash
    source DEF
    mkdir run
    ~angie/hummus/make-joblist $DEF > $BASE/run/j
    sh ./xdir.sh
    cd run
    sed -e 's#^#/cluster/bin/penn/#' j > j2
    wc -l j*
    head j2
    mv j2 j
    # cluster run -- use rack 9 to avoid getting in the way of hg17.
    ssh kk9
    cd /cluster/data/dm1/bed/blastz.anoGam1.2004-06-10/run
    para create j
    para try, check, push, check, ....
#Completed: 693 of 693 jobs
#Average job time:                 208s       3.47m     0.06h    0.00d
#Longest job:                      675s      11.25m     0.19h    0.01d
#Submission to last job:          1884s      31.40m     0.52h    0.02d

    # back on kksilo...
    mkdir /cluster/data/dm1/bed/blastz.anoGam1.2004-06-10/run.1
    cd /cluster/data/dm1/bed/blastz.anoGam1.2004-06-10/run.1
    ~angie/hummus/do.out2lav ../DEF > j
    # small cluster run
    ssh kki
    cd /cluster/data/dm1/bed/blastz.anoGam1.2004-06-10/run.1
    para create j
    para try, check, push, check, ....
#Completed: 21 of 21 jobs
#Average job time:                   5s       0.09m     0.00h    0.00d
#Longest job:                       12s       0.20m     0.00h    0.00d
#Submission to last job:            14s       0.23m     0.00h    0.00d
    cd ..
    rm -r raw

    # third run: lav -> axt
    ssh kki
    cd /cluster/data/dm1/bed/blastz.anoGam1.2004-06-10
    mkdir axtChrom pslChrom run.2
    cd run.2
    cat << '_EOF_' > do.csh
#!/bin/csh -ef
cd $1
set chr = $1:t
cat `ls -1 *.lav | sort -g` \
| $HOME/bin/x86_64/lavToAxt stdin \
    /cluster/bluearc/drosophila/dm1/nib /iscratch/i/anoGam1/nib stdout \
| $HOME/bin/x86_64/axtSort stdin ../../axtChrom/$chr.axt 
$HOME/bin/x86_64/axtToPsl ../../axtChrom/$chr.axt ../../S1.len ../../S2.len \
  ../../pslChrom/$chr.psl
'_EOF_'
    # << this line keeps emacs coloring happy
    chmod a+x do.csh
    cp /dev/null jobList
    foreach d (../lav/chr*)
      echo "do.csh $d" >> jobList
    end
    para create jobList
    para try, check, push, check
#Completed: 11 of 11 jobs
#Average job time:                   9s       0.16m     0.00h    0.00d
#Longest job:                       18s       0.30m     0.01h    0.00d
#Submission to last job:            18s       0.30m     0.01h    0.00d


# CHAIN ANOPHELES BLASTZ (DONE 6/10/04 angie)
    # Run axtChain on little cluster
    ssh kki
    cd /cluster/data/dm1/bed/blastz.anoGam1.2004-06-10
    mkdir -p axtChain/run1
    cd axtChain/run1
    mkdir out chain
    ls -1S /cluster/data/dm1/bed/blastz.anoGam1.2004-06-10/axtChrom/*.axt \
      > input.lst
    cat << '_EOF_' > gsub
#LOOP
doChain {check in exists $(path1)} {check out line+ chain/$(root1).chain} {check out exists out/$(root1).out}
#ENDLOOP
'_EOF_'
    # << this line makes emacs coloring happy

    # Make our own linear gap file with reduced gap penalties:
    cat << '_EOF_' > ../../chickenHumanTuned.gap
tablesize	11
smallSize	111
position	1	2	3	11	111	2111	12111	32111	72111	152111	252111
qGap	325	360	400	450	600	1100	3600	7600	15600	31600	56600
tGap	325	360	400	450	600	1100	3600	7600	15600	31600	56600
bothGap	625	660	700	750	900	1400	4000	8000	16000	32000	57000
'_EOF_'
    # << this line makes emacs coloring happy

    cat << '_EOF_' > doChain
#!/bin/csh
axtChain -scoreScheme=/cluster/data/blastz/HoxD55.q \
         -linearGap=../../chickenHumanTuned.gap \
         -verbose=0 $1 \
  /cluster/bluearc/drosophila/dm1/nib \
  /iscratch/i/anoGam1/nib $2 > $3
'_EOF_'
    # << this line makes emacs coloring happy
    chmod a+x doChain
    gensub2 input.lst single gsub jobList
    para create jobList
    para try, check, push, check...
#Completed: 11 of 11 jobs
#Average job time:                  13s       0.22m     0.00h    0.00d
#Longest job:                       19s       0.32m     0.01h    0.00d
#Submission to last job:            19s       0.32m     0.01h    0.00d

    # now on the cluster server, sort chains
    ssh kksilo
    cd /cluster/data/dm1/bed/blastz.anoGam1.2004-06-10/axtChain
    chainMergeSort run1/chain/*.chain > all.chain
    chainSplit chain all.chain
    rm run1/chain/*.chain

    # take a look at score distr's
    foreach f (chain/*.chain)
      grep chain $f | awk '{print $2;}' | sort -nr > /tmp/score.$f:t:r
      echo $f:t:r
      textHistogram -binSize=10000 /tmp/score.$f:t:r
      echo ""
    end

    # Load chains into database
    ssh hgwdev
    cd /cluster/data/dm1/bed/blastz.anoGam1.2004-06-10/axtChain/chain
    foreach i (*.chain)
        set c = $i:r
        echo loading $c
        hgLoadChain dm1 ${c}_chainAnoGam1 $i
    end


# NET ANOPHELES BLASTZ (DONE 6/10/04 angie)
    ssh kksilo
    cd /cluster/data/dm1/bed/blastz.anoGam1.2004-06-10/axtChain
    chainPreNet all.chain ../S1.len ../S2.len stdout \
    | chainNet stdin -minSpace=1 ../S1.len ../S2.len stdout /dev/null \
    | netSyntenic stdin noClass.net

    # Add classification info using db tables:
    ssh hgwdev
    cd /cluster/data/dm1/bed/blastz.anoGam1.2004-06-10/axtChain
    netClass -noAr noClass.net dm1 anoGam1 anopheles.net

    # Make a 'syntenic' subset:
    ssh kksilo
    cd /cluster/data/dm1/bed/blastz.anoGam1.2004-06-10/axtChain
    rm noClass.net
    netFilter -syn anopheles.net > anophelesSyn.net

    # Load the nets into database 
    ssh hgwdev
    cd /cluster/data/dm1/bed/blastz.anoGam1.2004-06-10/axtChain
    netFilter -minGap=10 anopheles.net |  hgLoadNet dm1 netAnoGam1 stdin
    netFilter -minGap=10 anophelesSyn.net | hgLoadNet dm1 netSyntenyAnoGam1 stdin


# MAKE VSANOGAM1 DOWNLOADABLES (DONE 6/17/04 angie)
    ssh kksilo
    cd /cluster/data/dm1/bed/blastz.anoGam1.2004-06-10/axtChain
    cp all.chain anopheles.chain
    zip /cluster/data/dm1/zips/anopheles.chain.zip anopheles.chain
    rm anopheles.chain
    zip /cluster/data/dm1/zips/anopheles.net.zip anopheles.net

    ssh hgwdev
    mkdir /usr/local/apache/htdocs/goldenPath/dm1/vsAnoGam1
    cd /usr/local/apache/htdocs/goldenPath/dm1/vsAnoGam1
    mv /cluster/data/dm1/zips/anopheles*.zip .
    md5sum *.zip > md5sum.txt
    # Copy over & edit README.txt w/pointers to chain, net formats.


# GENERATE ANOGAM1 MAF FOR MULTIZ FROM NET (DONE 6/10/04 angie)
    ssh kksilo
    cd /cluster/data/dm1/bed/blastz.anoGam1.2004-06-10/axtChain
    netSplit anopheles.net net
    ssh kolossus
    cd /cluster/data/dm1/bed/blastz.anoGam1.2004-06-10
    mkdir axtNet
    foreach f (axtChain/net/*)
      set chr = $f:t:r
      netToAxt $f axtChain/chain/$chr.chain /cluster/data/dm1/nib \
        /cluster/data/anoGam1/nib stdout \
      | axtSort stdin axtNet/$chr.axt
    end

    mkdir mafNet
    foreach f (axtNet/chr*.axt)
      set maf = mafNet/$f:t:r.ma.maf
      axtToMaf $f \
            /cluster/data/dm1/chrom.sizes /cluster/data/anoGam1/chrom.sizes \
            $maf -tPrefix=dm1. -qPrefix=anoGam1.
    end


# MULTIZ MELANOGASTER/YAKUBA/PSEUDOOBSCURA/ANOPHELES (DONE 6/12/04 angie)
    # put the MAFs on bluearc
    ssh kksilo
    mkdir -p /cluster/bluearc/multiz.flymo/myp
    cp /cluster/data/dm1/bed/multiz.dm1droYak1dp1/myp/*.maf \
      /cluster/bluearc/multiz.flymo/myp
    mkdir -p /cluster/bluearc/multiz.flymo/ma
    cp /cluster/data/dm1/bed/blastz.anoGam1.2004-06-10/mafNet/*.maf \
      /cluster/bluearc/multiz.flymo/ma

    ssh kki
    mkdir /cluster/data/dm1/bed/multiz.dm1droYak1dp1anoGam1
    cd /cluster/data/dm1/bed/multiz.dm1droYak1dp1anoGam1
    mkdir mypa
    # Wrapper script required because of stdout redirect:
    cat << '_EOF_' > doMultiz
#!/bin/csh
/cluster/bin/penn/multiz $1 $2 - > $3
'_EOF_'
    # << this line makes emacs coloring happy
    chmod a+x doMultiz
    rm -f jobList
    foreach file (/cluster/bluearc/multiz.flymo/myp/*.maf) 
      set root=$file:t:r:r
      echo "doMultiz /cluster/bluearc/multiz.flymo/ma/${root}.ma.maf $file /cluster/data/dm1/bed/multiz.dm1droYak1dp1anoGam1/mypa/${root}.maf" >> jobList
    end
    para create jobList
    para try, check, push, check
#Completed: 11 of 11 jobs
#Average job time:                  25s       0.42m     0.01h    0.00d
#Longest job:                       55s       0.92m     0.02h    0.00d
#Submission to last job:            55s       0.92m     0.02h    0.00d
    du -sh mypa
#410M    mypa

    # clean up bluearc
    rm -r /cluster/bluearc/multiz.flymo

    # zip it for download
    zip -j /cluster/data/dm1/zips/multizDm1DroYak1Dp1AnoGam1.zip mypa/*.maf

    # setup external files for database reference
    ssh hgwdev
    mkdir /gbdb/dm1/mzDy1Dp1Ag1_phast
    ln -s /cluster/data/dm1/bed/multiz.dm1droYak1dp1anoGam1/mypa/chr*.maf \
      /gbdb/dm1/mzDy1Dp1Ag1_phast/
    # load into database
    hgLoadMaf -warn dm1 mzDy1Dp1Ag1_phast
    # load pairwise mafs for wigMaf track
    cd /gbdb/dm1
    mkdir -p d_yakuba_mypa d_pseudoobscura_mypa a_gambiae_mypa
    cd /tmp
    ln -s /cluster/data/dm1/bed/blastz.droYak1.2004-05-22/mafNet/*.maf \
      /gbdb/dm1/d_yakuba_mypa
    hgLoadMaf -WARN dm1 d_yakuba_mypa
    ln -s /cluster/data/dm1/bed/blastz.dp1.2003-11-14/mafNet/*.maf \
      /gbdb/dm1/d_pseudoobscura_mypa
    hgLoadMaf -WARN dm1 d_pseudoobscura_mypa
    ln -s /cluster/data/dm1/bed/blastz.anoGam1.2004-06-10/mafNet/*.maf \
      /gbdb/dm1/a_gambiae_mypa
    hgLoadMaf -WARN dm1 a_gambiae_mypa

    # put it out for download
    mkdir /usr/local/apache/htdocs/goldenPath/dm1/multizDroYak1Dp1AnoGam1
    cd /usr/local/apache/htdocs/goldenPath/dm1/multizDroYak1Dp1AnoGam1
    mv /cluster/data/dm1/zips/multizDm1DroYak1Dp1AnoGam1.zip .
    md5sum *.zip > md5sum.txt


# PHASTCONS MELANOGASTER/YAKUBA/PSEUDOOBSCURA (first cut 6/1/04 acs)

    # make sure /cluster/bin/phast is in your path
    # commands below use bash shell
    # watch for typos -- copying from a Makefile and editing a bit

    # there are three steps to go through: fitting a phylogenetic
    # model to the data set with phyloFit, then running phastCons for
    # conservation scores, then running phastCons for predictions of
    # conserved elements.

    # step 1: fit phylogenetic model, with rate variation (discrete
    # gamma model) first we need to extract the sufficient stats for
    # the phylogenetic model from the MAFs.  In this case, we don't
    # care about the order of the tuples (allows for much more compact
    # representation)

    ssh eieio
    mkdir /cluster/data/dm1/bed/phastCons
    cd /cluster/data/dm1/bed/phastCons
    MAF=/cluster/data/dm1/bed/multiz.dm1droYak1dp1/myp
    # first extract chromosome by chromosome
    for file in ${MAF}/*.maf ; do \
	msa_view $file --in-format MAF --out-format SS --order dm1,droYak1,dp1 \
	    --unordered-ss  > `basename $file .maf`.ss ;\
    done
    # now aggregate for whole genome
    ls chr*.ss > fnames
    msa_view "*fnames" -i SS --aggregate dm1,droYak1,dp1 -o SS --unordered-ss > all.ss

    # now estimate parameters; should be very fast in this case
    phyloFit --msa all.ss --tree "((1,2),3)" --msa-format SS --subst-mod REV --nrates 10 --out-root rev-dg --output-tree

    # make sure model looks reasonable
    cat rev-dg.mod
#ALPHABET: A C G T
#ORDER: 0
#SUBST_MOD: REV
#NRATECATS: 10
#ALPHA: 1.273963
#TRAINING_LNL: -290963642.026431
#BACKGROUND: 0.279634 0.220483 0.220439 0.279444
#RATE_MAT:
#  -0.929128    0.196259    0.495954    0.236916
#   0.248910   -1.089402    0.212511    0.627981
#   0.629134    0.212554   -1.090600    0.248913
#   0.237077    0.495481    0.196354   -0.928911
#TREE: ((1:0.057640,2:0.073631):0.166542,3:0.166542);

    # beware of zero branch lengths (indicates bad topology) or very
    # large branch lengths (much greater than one; indicates
    # convergence problems).  Can try without --nrates or with
    # --subst-mod HKY85 to double check (parameter estimates should be
    # in the same ballpark, REV + dG likelihood should be a bit
    # better).  Also can write a log file to monitor convergence
    # (--log).  Also watch out for alpha much less than one.
    # Sometimes it's helpful to produce a rendering of the tree using
    # draw_tree (run on the *.nh file)

    # step 2: run phastCons.  First partition the alignments into
    # bite-sized chunks.  This time we need the ordered version of the
    # SS format.

    # some vars used below
    MAF=/cluster/data/dm1/bed/multiz.dm1droYak1dp1/myp
    FA=/cluster/data/dm1
    WINSIZE=1000000
    WINOVERLAP=0

    for file in ${MAF}/*.maf ; do \
	root=`basename $file .maf` ;\
	chr=`echo $root | sed 's/chr//'` ;\
	echo $file $root $chr ;\
	mkdir -p SS/$chr ;\
	msa_split $file -i MAF -o SS -O dm1,droYak1,dp1 -M ${FA}/$chr/$root.fa \
	    -w ${WINSIZE},${WINOVERLAP} -r SS/$chr/$$root -I 1000 -d 1 -B 5000 ;\
    done
    # (this is worth doing as a little cluster job with mammalian genomes,
    # but it's pretty fast with fly)

    ssh hgwdev
    cd /cluster/data/dm1/bed/phastCons

    # now set up cluster job.  Make a little wrapper for phastCons
    cat << '_EOF_' > doPostProbs
#!/bin/sh

PHAST=/cluster/bin/phast
TMP=/scratch/phastCons

file=$1
root=`basename $file .ss`
chrom=`echo $root | awk -F\. '{print $1}'`

mkdir -p $TMP
$PHAST/phastCons $file rev-dg.mod --cut-at 2 --nrates 10 --suppress-missing --transitions 0.030,0.015 --quiet > ${TMP}/$root.pp
mkdir -p POSTPROBS/$chrom
gzip -c $TMP/$root.pp > POSTPROBS/$chrom/$root.pp.gz
rm $TMP/$root.pp
'_EOF_'
    # << this line makes emacs coloring happy
    chmod 775 doPostProbs

    # Note: the --cut-params arguments above are approximate
    # likelihood estimates obtained by running phastCons *without* the
    # --cut-params argument on four or five different windows (all
    # gave similar results).  They may need to change for different
    # data sets.  Careful, though: the parameter estimation procedure
    # is a little unstable.  Be sure to create a log file and monitor
    # for convergence.

    # 2nd note: manually tried various parameter settings and settled
    # on --cut-params 0.010,0.005.  Seems to give best results for
    # both post probs and viterbi path.  At least until we settle on a
    # better principle for setting the params, these are the
    # recommended ones to use.

    # set up a jobs list
    rm -f jobs.lst
    for file in `find SS -name "*.ss"` ; do echo doPostProbs $file >> jobs.lst ; done

    # run cluster job
    ssh kk
    cd /cluster/data/dm1/bed/phastCons
    para create jobs.lst ; para try ; para push ; etc....
#Completed: 134 of 134 jobs
#CPU time in finished jobs:       2584s      43.07m     0.72h    0.03d  0.000 y
#IO & Wait Time:                  2090s      34.83m     0.58h    0.02d  0.000 y
#Average job time:                  35s       0.58m     0.01h    0.00d
#Longest job:                       46s       0.77m     0.01h    0.00d
#Submission to last job:            62s       1.03m     0.02h    0.00d
    logout

    # now create wiggle track
    # NOTE: might want to integrate with the multiz track instead of
    # keeping separate
    mkdir -p wib
    for dir in POSTPROBS/* ; do \
	echo $dir ;\
	chr=`basename $dir` ;\
	zcat `ls $dir/*.pp.gz | sort -t\. -k2,2n` | \
	    wigAsciiToBinary -chrom=$chr \
	    -wibFile=wib/${chr}_phastCons stdin ;\
    done
    hgLoadWiggle dm1 phastCons wib/chr*_phastCons.wig
    mkdir -p /gbdb/dm1/wib
    rm -f /gbdb/dm1/wib/*phastCons.wib
    ln -s `pwd`/wib/*.wib /gbdb/dm1/wib
    chmod 775 . wib
    chmod 664 wib/*.wib

    # trackDb.ra entry
#track phastCons
#shortLabel phastCons 
#longLabel phastCons Conservation Score, melanogaster/yakuba/pseudoobscura
#group compGeno
#priority 103
#visibility hide
#color 0,10,100
#maxHeightPixels 40
#type wig 0.0 1.0
#autoScaleDefault off

    # step 3:  predictions of conserved elements
    # (could do these at the same time as step 2, but we want to use
    # different --rates-cut params)
    cat << '_EOF_' > doViterbi
#!/bin/sh

PHAST=/cluster/home/acs/phast/bin
TMP=/scratch/phastCons

file=$1
root=`basename $file .ss`
chrom=`echo $root | awk -F\. '{print $1}'`

mkdir -p PREDICTIONS/$chrom
$PHAST/phastCons $file rev-dg.mod --nrates 10 --viterbi PREDICTIONS/$chrom/$root.bed --score --no-post-probs --transitions 0.030,0.015 --quiet --seqname $chrom
'_EOF_'
    # << this line makes emacs coloring happy
    chmod 775 doViterbi

    # see note above regarding --cut-params (0.010,0.005 recommended)

    rm -f jobs.viterbi.lst
    for file in `find SS -name "*.ss"` ; do echo doViterbi $file >> jobs.viterbi.lst ; done
    logout

    ssh kk
    cd /cluster/data/dm1/bed/phastCons
    para create jobs.viterbi.lst ; para try ; para push ; etc....
#CPU time in finished jobs:        320s       5.34m     0.09h    0.00d  0.000 y
#IO & Wait Time:                   427s       7.11m     0.12h    0.00d  0.000 y
#Average job time:                   6s       0.09m     0.00h    0.00d
#Longest job:                       10s       0.17m     0.00h    0.00d
#Submission to last job:            52s       0.87m     0.01h    0.00d
    logout

    # create track; we want to tweak the scores and the names
    sed 's/id //' PREDICTIONS/*/*.bed | \
	awk '{printf "%s\t%s\t%s\tlod=%d\t%d\t%s\t%s\t%s\t%s\t%s\t%s\t%s\n", \
	    $1, $2, $3, $5, 221.65 * log($5) - 352.64, $6, $7, $8, $9, \
	    $10, $11, $12}' > all.bed
    hgLoadBed dm1 phastConsElements all.bed

    # Scores are transformed as follows, for a reasonable-looking 
    # "spectrum".  Let x_max be the maximum (believable) score (here
    # x_max  = 447) and let x_med be the median score (here x_med =
    # 19).  The  scores are transformed via the function f(x) = a *
    # log x + b, s.t. f(x_med) = 300 and f(x_max) = 1000.  Solving
    # for a and b, you obtain  b = (300 log x_max - 1000 log x_med) /
    # (log x_max - log x_med), a = (1000 - b) / log x_max.  Here a =
    # 221.65, b = -352.64.

    # trackDb.ra file
#track phastConsElements
#shortLabel phastConsElements
#longLabel phastCons Conserved Elements, melanogaster/yakuba/pseudoobscura
#group compGeno
#priority 105
#visibility hide
#spectrum on
#color 0,60,120
#altColor 200,220,255
#exonArrows off
#type bed 12 .

    # should gzip or even delete contents of SS directory when done


# PHASTCONS MELANOGASTER/YAKUBA/PSEUDOOBSCURA/ANOPHELES (DONE 6/15/04 angie)
    # step 1: fit phylogenetic model, with rate variation (discrete
    # gamma model) first we need to extract the sufficient stats for
    # the phylogenetic model from the MAFs.  In this case, we don't
    # care about the order of the tuples (allows for much more compact
    # representation)
    ssh kksilo
    mkdir /cluster/data/dm1/bed/phastCons4way
    cd /cluster/data/dm1/bed/phastCons4way
    mkdir unordered
    foreach f (/cluster/data/dm1/bed/multiz.dm1droYak1dp1anoGam1/mypa/chr*.maf)
      /cluster/bin/phast/msa_view $f --in-format MAF --out-format SS \
            --order dm1,droYak1,dp1,anoGam1 \
            --unordered-ss  > unordered/$f:t:r.ss
    end
    # aggregate for whole genome
    ls unordered/chr*.ss > fnames
    /cluster/bin/phast/msa_view "*fnames" -i SS \
      --aggregate dm1,droYak1,dp1,anoGam1 -o SS --unordered-ss \
      > all.ss
    # estimate parameters; should be very fast in this case
    /cluster/bin/phast/phyloFit --msa all.ss --tree "(((1,2),3),4)" \
      --msa-format SS --subst-mod REV --nrates 10 --out-root rev-dg \
      --output-tree
    # make sure model looks reasonable
    cat rev-dg.mod
#ALPHABET: A C G T 
#ORDER: 0
#SUBST_MOD: REV
#NRATECATS: 10
#ALPHA: 1.342554
#TRAINING_LNL: -311913607.826812
#BACKGROUND: 0.276944 0.223186 0.223135 0.276735 
#RATE_MAT:
#  -0.926228    0.197669    0.496394    0.232165 
#   0.245281   -1.090732    0.230621    0.614830 
#   0.616100    0.230674   -1.092338    0.245564 
#   0.232341    0.495858    0.198001   -0.926200 
#TREE: (((1:0.056495,2:0.074413):0.111244,3:0.219007):0.288671,4:0.288671);

    # Since anopheles is at such a large distance (est. divergence of 
    # branches that became drosophila & anopheles: ~250mya, as opposed to 
    # 35-40mya for mel-pseudo and 10-15mya for mel-yakuba), the branch length
    # between anopheles and pseudoobscura is underestimated above.  Adam 
    # scaled the lengths according to the published est. mya's, so 
    # edit rev-dg.mod's TREE line to this:
#TREE: (((1:0.058,2:0.074):0.133,3:0.200):0,4:2.66);

    # beware of zero branch lengths (indicates bad topology) or very
    # large branch lengths (much greater than one; indicates
    # convergence problems).  Can try without --nrates or with
    # --subst-mod HKY85 to double check (parameter estimates should be
    # in the same ballpark, REV + dG likelihood should be a bit
    # better).  Also can write a log file to monitor convergence
    # (--log).  Also watch out for alpha much less than one.
    # Sometimes it's helpful to produce a rendering of the tree using
    # draw_tree (run on the *.nh file)

    # step 2: run phastCons.  First partition the alignments into
    # bite-sized chunks.  This time we need the ordered version of the
    # SS format.
    set WINSIZE=1000000
    set WINOVERLAP=0
    mkdir SS
    foreach f (/cluster/data/dm1/bed/multiz.dm1droYak1dp1anoGam1/mypa/*.maf)
      set chr=$f:t:r
      set c=`echo $chr | sed 's/chr//'`
      echo $f $chr $c
      mkdir SS/$c
      /cluster/bin/phast/msa_split $f -i MAF -o SS \
        -O dm1,droYak1,dp1,anoGam1 -M /cluster/data/dm1/$c/$chr.fa \
        -w ${WINSIZE},${WINOVERLAP} -r SS/$c/$chr -I 1000 -d 1 -B 5000
    end

    ssh kk
    cd /cluster/data/dm1/bed/phastCons4way
    cat << '_EOF_' > doPostProbs
#!/bin/sh

PHAST=/cluster/bin/phast
TMP=/scratch/phastCons

file=$1
root=`basename $file .ss`
chrom=`echo $root | awk -F\. '{print $1}'`

mkdir -p $TMP
$PHAST/phastCons $file rev-dg.mod --cut-at 2 --nrates 10 --suppress-missing --transitions 0.030,0.015 --quiet > ${TMP}/$root.pp
mkdir -p POSTPROBS/$chrom
gzip -c $TMP/$root.pp > POSTPROBS/$chrom/$root.pp.gz
rm $TMP/$root.pp
'_EOF_'
    # << this line makes emacs coloring happy
    chmod 775 doPostProbs
    # Note: the --cut-params arguments above are approximate
    # likelihood estimates obtained by running phastCons *without* the
    # --cut-params argument on four or five different windows (all
    # gave similar results).  They may need to change for different
    # data sets.  Careful, though: the parameter estimation procedure
    # is a little unstable.  Be sure to create a log file and monitor
    # for convergence.
    rm -f jobs.lst
    foreach f (`ls -1S SS/*/*.ss`)
      echo './doPostProbs {check in line+ '$f'}' \
      >> jobs.lst
    end
    para create jobs.lst 
    para try ; para push ; etc....
#Completed: 134 of 134 jobs
#Average job time:                  31s       0.51m     0.01h    0.00d
#Longest job:                       50s       0.83m     0.01h    0.00d
#Submission to last job:            87s       1.45m     0.02h    0.00d

    # now create wiggle track
    # NOTE: might want to integrate with the multiz track instead of
    # keeping separate
    ssh kksilo
    cd /cluster/data/dm1/bed/phastCons4way
    mkdir wib
    foreach dir (POSTPROBS/*)
        echo $dir
        set chr=$dir:t
        zcat `ls -1 $dir/*.pp.gz | sort -t\. -k2,2n` | \
            wigAsciiToBinary -chrom=$chr \
            -wibFile=wib/${chr}_phastCons4way stdin
    end
    ssh hgwdev
    mkdir -p /gbdb/dm1/wib/mzDy1Dp1Ag1_phast
    cd /cluster/data/dm1/bed/phastCons4way
    chmod 775 . wib
    chmod 664 wib/*.wib
    ln -s `pwd`/wib/*.wib /gbdb/dm1/wib/mzDy1Dp1Ag1_phast/
    hgLoadWiggle dm1 mzDy1Dp1Ag1_phast_wig \
      -pathPrefix=/gbdb/dm1/wib/mzDy1Dp1Ag1_phast wib/*.wig

    # step 3:  predictions of conserved elements
    # (could do these at the same time as step 2, but we want to use
    # different --rates-cut params)
    ssh kk
    cd /cluster/data/dm1/bed/phastCons4way
    cat << '_EOF_' > doViterbi
#!/bin/sh

PHAST=/cluster/home/acs/phast/bin
TMP=/scratch/phastCons

file=$1
root=`basename $file .ss`
chrom=`echo $root | awk -F\. '{print $1}'`

mkdir -p PREDICTIONS/$chrom
$PHAST/phastCons $file rev-dg.mod --nrates 10 --viterbi PREDICTIONS/$chrom/$root.bed --score --no-post-probs --transitions 0.030,0.015 --quiet --seqname $chrom
'_EOF_'
    # << this line makes emacs coloring happy
    chmod 775 doViterbi
    rm -f jobs.viterbi.lst
    foreach f (SS/*/*.ss)
      echo 'doViterbi {check in line+ '$f'}' >> jobs.viterbi.lst
    end
    para create jobs.viterbi.lst
    para try ; para push ; etc....
#Completed: 134 of 134 jobs
#Average job time:                   5s       0.09m     0.00h    0.00d
#Longest job:                        9s       0.15m     0.00h    0.00d
#Submission to last job:            39s       0.65m     0.01h    0.00d

    # Create bed track; we want to tweak the scores and the names.
    # Scores are transformed as follows, for a reasonable-looking 
    # bed 0-1000 "spectrum".  Let x_max be the maximum (believable) score 
    # (here x_max  = 1827) and let x_med be the median score (here x_med =
    # 20).  The  scores are transformed via the function f(x) = a *
    # log x + b, s.t. f(x_med) = 300 and f(x_max) = 1000.  Solving
    # for a and b, you obtain  b = (300 log x_max - 1000 log x_med) /
    # (log x_max - log x_med), a = (1000 - b) / log x_max.  Here a =
    # 232.024, b = -742.603.
    # better done on kksilo:
    # get max:
    set max = `awk '{print $6;}' PREDICTIONS/*/*.bed | sort -n | tail -1`
#1827
    # get median:
    set lineCount = `cat PREDICTIONS/*/*.bed | wc -l`
    set medOffset = `expr '(' $lineCount / 2 ')' + 1`
    set med = `awk '{print $6;}' PREDICTIONS/*/*.bed | sort -n \
               | tail +$medOffset | head -1`
#20
    # b and a:
    set b = `awk 'BEGIN {print ((300 * log('$max')) - (1000 * log('$med')));}'`
#-742.603
    set a = `awk 'BEGIN {print (1000 - '$b') / log('$max');}'`
#232.024
    sed 's/id //' PREDICTIONS/*/*.bed | \
        awk '{printf "%s\t%s\t%s\tlod=%d\t%d\t%s\t%s\t%s\t%s\t%s\t%s\t%s\n", \
            $1, $2, $3, $5, '$a' * log($5) + '$b', $6, $7, $8, $9, \
            $10, $11, $12}' > all.bed
    hgLoadBed dm1 phastCons4wayElements all.bed

    # should gzip or even delete contents of SS directory when done
    nice gzip SS/*/*.ss &

    # make top-5000 list and launcher on Adam's home page:
    sed 's/id //' PREDICTIONS/*/*.bed | sort -k5,5nr | head -5000 \
    > top5000.bed
    /cluster/home/acs/bin/make-launcher-with-scores.sh top5000.bed \
      /cse/grads/acs/public_html/dm-top5000-4way \
      "top 5000 conserved elements (4way)" dm1


# LOAD ENSEMBL GENES (NEVER MIND 6/16/04 angie)
    # OK, never mind, Ensembl just imported the BDGP / Flybase genes.


